<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width initial-scale=1" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>Machine Learning - MRI Analysis Project (Part II)</title>
    <meta name="description" content="Personal Website and Blog
">

    <link rel="stylesheet" href="/css/main.css">
    <link rel="canonical" href="https://www.martinholub.com/2017/06/05/MachineLearningPart2.html">

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css">
	<link rel="stylesheet" href="/css/academicons.css"/>
    <link href="/css/lightbox.css" rel="stylesheet">

    <link rel="stylesheet" href="/css/zenburn.css"/>

    <!-- collect tags -->
    
      <!-- collect tags from blog posts -->
<!-- source: https://longqian.me/2017/02/09/github-jekyll-tag/ --->







    

	<link type="application/atom+xml" rel="alternate" href="https://www.martinholub.com/feed.xml" title="Martin Holub Blog" />
</head>

  
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-1ZBQKKLMRB"></script>
  <script>
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());

	gtag('config', 'G-1ZBQKKLMRB');
  </script>

  <script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
	</script>

  <body>

    <header class="site-header">

  <div class="wrapper">

    <nav class="site-nav">

      <div class="trigger">
        <!-- Martin Holub Blog instead of blog -->
        <a class="page-link" href="/">about</a>

        <!-- ( for page in site.pages )  was previously-->
        
          
          <a class="page-link" href="/blog">blog</a>
          
        
          
          <a class="page-link" href="/research">research</a>
          
        
          
          <a class="page-link" href="/projects">projects</a>
          
        
          
          <a class="page-link" href="/resources">resources</a>
          
        
          
          <a class="page-link" href="/subscribe">subscribe</a>
          
        

      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Machine Learning - MRI Analysis Project (Part II)</h1>
    <p class="post-meta">June 5, 2017 — 15:22 • <a href="https://martinholub.github.io/2017/06/05/MachineLearningPart2.html#disqus_thread">0 Comments</a></p>
    <span class="post-meta">[
      
        
          <a href="/tag/ML"><code class="highligher-rouge"><nobr>ML</nobr></code>&nbsp;</a>
      
        
          <a href="/tag/projects"><code class="highligher-rouge"><nobr>projects</nobr></code>&nbsp;</a>
      
        
          <a href="/tag/ethz"><code class="highligher-rouge"><nobr>ethz</nobr></code>&nbsp;</a>
      
        
          <a href="/tag/uni"><code class="highligher-rouge"><nobr>uni</nobr></code>&nbsp;</a>
      
    ]</span>
  </header>
  

  <article class="post-content">
    <div class="img_row" style="height: 275px;">
	<img class="col three" src="/img/MachineLearningHeader.jpg" alt="" title="MachineLearningHeader" />
</div>
<div class="col three caption">
</div>

<p><i>
<em>This is a second part of an introductory post on machine learning. It is based on a practical project I worked on at ETH. Here I give you an explanation of two crucial steps in machine learning pipeline – Model selection and Validation. I then wrap up with some useful takeaways from the project, that you can eventually use in your own work and concise summary. If you missed the first part of the post, or just feel you could use a refresher, you can find it <a href="http://www.martinholub.com/2017/05/07/MachineLearningPart1.html">here</a>.</em></i></p>
<hr />

<p><br /></p>

<h3 id="model-selection">Model Selection</h3>
<p><br />
Although we have discussed quite few steps so far, they were only preparatory (albeit necessary and crucial) and it is only now, in model selection phase, that we actually get our hands on machine learning algorithms. We can further divide this phase into two steps – training and validation. Let’s look at training first.
In the scope of our project we needed to deal with both <a href="http://www.investopedia.com/terms/r/regression.asp">regression</a> and <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification</a>, here I will describe the latter one as it is perhaps more intuitive and again, I will illustrate it on an example from our project. Recalling the task at hand – to disambiguate between mentally healthy and sick patients from an MRI scan, we can imagine a simplified scenario where each brain scan is described by single two-dimensional vector of features. Further, assume that they form two point-clouds that are perfectly linearly separable as show in the figure:
<!---break---></p>
<div class="img_row">
	<img class="col three" src="/img/idealizedClassification.png" alt="" title="idealizedClassification" style="height: 100%; width: 100%; object-fit: contain" />
</div>
<div class="col three caption"> Fig. 5: Idealized example of classification task </div>

<div class="img_row">
	<img class="col three" src="/img/realisticClassification.png" alt="" title="realisticClassification" style="height: 100%; width: 100%; object-fit: contain" />
</div>
<div class="col three caption"> Fig. 6: More realistic example of a classification task, notice that data don't form any recognizable sub-groups </div>

<p>We may than write down an equation of line that divides the two-dimensional space into two half-planes. Whenever we make a new observation (take an MRI of a new patient), his brain, described by two <a href="https://en.wikipedia.org/wiki/Feature_vector">feature vector</a>, will fall to one of those two regions and we will label it accordingly as ‘healthy’ or ‘sick’.</p>

<p>Now the situation is usually by far not that simple. First, recall that our feature vectors are n-dimensional, where n is of the order of hundreds (more precisely \(27 \times 35\ = 945\)). We may try projecting our point-clouds onto 2 dimensions, but we will likely end up with something looking like what is shown in figure above. It follows intuitively that we will have to go for something more complex than a line to separate these clusters. Nevertheless, mathematically there is no fundamental difference to how we will attempt to solve the task.</p>

<p>In the most general terms, proceeding with the classification example, we look for a function that, given input vector, outputs a prediction, i.e. in this case either 0 or 1, which minimizes some objective. This function is defined by set of parameters. The common objective to minimize is so called <a href="https://en.wikipedia.org/wiki/Loss_function">loss function</a> which takes our prediction, true label and outputs a value that represents a penalization for misclassification. In mathematical notation, this is expressed as follows:
\(\mathbf{\theta^{*}} = argmin_{\theta}\mathcal{L}(f(\textbf{X}, \mathbf{\theta}); \textbf{t}),\)</p>

<p>Where \(\mathbf{\theta^{*}}\) is the vector of optimal parameters, \(\mathcal{L}(\cdot)\) is a loss function, \(f(\cdot)\) is a function that gives us our prediction, \(X\) is a matrix of dimensions N×D, N and D being number of samples and features respectively, and \(t\) is a vector of true labels.</p>

<p>Let’s look at an example to understand this better. If the patient is healthy and we predict 0 (0 meaning no sickness), then the loss will be zero. Contrary, when we would predict 1, meaning presence of sickness, then the loss will be some nonzero value. If we take a step back and look at the problem for a while, we will easily see that we are attempting no more than function-fitting here. Of course, the number of dimensions, accessory constraints, further elements of the pipeline and the richness of the machine learning field make it rather a fancy kind of function fitting. This is nevertheless to show, that one shouldn’t expect magic to happen when confronted with machine learning algorithms. It is just math that happens to receive a lot of hype.</p>

<h3 id="validation">Validation</h3>
<p><br />
Let’s proceed now to the second part of model selection phase – <a href="http://machinelearningmastery.com/how-to-evaluate-machine-learning-algorithms/">validation</a>. As you might have noticed, the ideal predictor function one would obtain from training phase is the one that gives 0 training loss, meaning that all samples were classified correctly. This appears desirable, but only to a point when one takes a look at decision boundaries of such a function. For illustration, let’s look how they may look like in such a case:</p>

<div class="img_row">
	<img class="col three" src="/img/overfitting.png" alt="" title="overfitting" style="height: 100%; width: 100%; object-fit: contain" />
</div>
<div class="col three caption"> Fig. 7: Class boundaries after minimizing training error, notice that although the error is zero, the result isn't satisfactory as it is a case of overfitting (<a href="http://openclassroom.stanford.edu/MainFolder/courses/MachineLearning/exercises/ex8ma">Image Source</a>) </div>

<p>As can be seen, even if the function is optimal in a sense of giving lowest possible training loss (exactly zero as all points are classified correctly), it is by far not a good representation of the underlying structure in our data. Imagine trying to classify a new patient (ie. datapoint in this 2D plot) that falls into a region where the blue question mark is. Intuitively we would assign him among healthy patients, contrary to the, the classifier we trained will label it as sick. In other terms, the model we have obtained from training phase does poor job <a href="https://www.quora.com/What-is-generalization-in-machine-learning">generalizing</a> to new observations, we also say that we have “overfitted to training data”. It is such a prominent pitfall in machine learning that there is even an <a href="https://www.youtube.com/watch?v=DQWI1kvmwRg">Acappella song</a> to help you remember it and avoid it.</p>

<p>There are various measures one can take to address this issue. Their nature lies in estimating the <a href="https://en.wikipedia.org/wiki/Generalization_error">generalization error</a> of a model obtained in training phase and selection of such a model that minimizes it. Thus, one would usually define several models to be trained and then evaluate their performance on unseen samples to estimate generalization error and then opt for the one that is the most consistent, i.e. that generalizes the best. Here we come back to how useful it is to have ample data. In ideal case scenario, we would train a model on part of our dataset and evaluate it on another, that wasn’t used in training. The issue with this approach is that only rarely one has enough data to do it. This is because by holding out data from training, we arrive to a model that is too simple, we also say it has a high <a href="https://en.wikipedia.org/wiki/Bias_of_an_estimator">bias</a>. To attenuate this, we look for approaches that put aside only small portion of samples for training from dataset. To still provide good estimate of the generalization error, we do this repeatedly and track the total error.</p>

<p>Let’s take a look at an example from our project again and let’s focus only on one type of a model to be trained. This still allows the situation at hand to be sufficiently complex as practically every model type is parametrized by a set of so called <a href="https://www.quora.com/What-are-hyperparameters-in-machine-learning">hyperparameters</a>, effectively yielding whole family of models that are to be first trained and then validated. In the case of our project, and again, considering classification task, we most frequently used <a href="https://en.wikipedia.org/wiki/Support_vector_machine">Support Vector Machine</a> Classifier with <a href="https://en.wikipedia.org/wiki/Radial_basis_function">Radial-basis function</a> kernel (if you are getting lost in terminology, don’t worry, radial-basis function kernel is just a function that measures distance between points). This model has two hyperparameters, namely penalty and bandwidth (In brief these indicate how tolerant with respect to a point that gets misclassified we are and how much is our labeling of a datapoint influenced by its neighbors). We than defined range of possible values for those parameters. The rest of the job, we offloaded to off-the-shelf validation procedure <a href="http://scikit-learn.org/stable/modules/grid_search.html">GridSearchCV</a> from <a href="http://scikit-learn.org/stable/index.html">scikit-learn</a> (great Python module featuring wide array of tools for machine learning). This routine enables one to select combinations of hyperparameters and strategy for selecting samples for validation – for this we selected 10-fold cross validation (This means that in each run, model is trained on 9/10 of data and 1/10 is left out for validation. The data are reassigned after each run so that after 10 runs all samples were used exactly once for validation.)</p>

<p>The GridSearch routine outputs a validation error for each model it is provided with. To increase <a href="https://www.researchgate.net/post/What_is_the_definition_of_the_robustness_of_a_machine_learning_algorithm">robustness</a> (i.e. to make the model generalize better on unseen data), we ran it several times on randomly shuffled samples and calculated <a href="http://www.robertniles.com/stats/stdev.shtml">standard deviation</a> among runs. As a final selection measure, we used \(mean\ error \times standard\ deviation\) among runs. The model that minimizes this metric is selected for final classification.</p>

<h3 id="results-and-submission">Results and Submission</h3>
<p><br />
At this point, we are basically finished with the task as described above. In the case of our project this means that we can save our predictions to a file and submit it to <a href="https://inclass.kaggle.com/">Kaggle</a> (an online platform for administering big machine learning projects and competitions that hosts also university projects) to obtain a score on how well did we fare.</p>

<p>Although this sounds trivial, there are still some things to keep an eye on. First, the score is computed based on another portion of dataset that one isn’t allowed to access. As we are allowed to submit the solution repeatedly, it is inevitable (the less experience one has the more likely so) to try to achieve the best score. This in practice means, that there is leakage of information from the ‘hidden’ data to the model being trained. And this in turn means that we are on the best track to overfit again. This is because Kaggle uses only one half of the held-out dataset to compute the score, and usually the more we try to optimize on the available half, the worse the final score, that is based on the other half, will be. In the first two parts of the project, we made exactly this mistake, and although our predictions scored well on public leaderboard, we did poorly on the private one. For the last part of the project, we increased the robustness of model selection procedure (for example by implementing the GridSearchCV, tracking standard deviation among runs of random permutation of data, etc…) and this allowed us to perform well above average.</p>

<h3 id="remarks-and-takeaway">Remarks and Takeaway</h3>
<p><br />
As a final part of this post, I will share with you some tips regarding the just-described procedure. Those are the things I wish I knew at the beginning at the project, because they would save us many valuable hours. My hope is that you can make use of these tips in some of your project of your own.</p>

<ul>
  <li>In our team, we were three people that didn’t share much classes together, this means that we didn’t get to talk to each other very often in person, which made it complicated to keep everybody on the same page and contributing more or less equally to the task. Whenever I would be to go into such a project again, I would make use of some online available task/project management tool (something we unluckily didn’t do) to track individual assignments and progress. I believe that this will make the project more enjoyable and the team more effective.</li>
  <li>As you could have noticed in Feature Selection section, getting good features is both crucial and sort of alchemy. One will often spend significant amount of time looking for good features and even an ample experience will usually not make this process straightforward. The recent developments in the field try to go in the opposite direction. Instead of laboriously engineering features, they consider them as part of a model to be learned. So called <a href="http://cs231n.github.io/convolutional-networks/">Convolutional Neural Networks</a> have already outperformed the best approaches with manually designed features and thus represent attractive choices in this scenario. We haven’t used them in our project though, as for a good feature to be learned one needs to provide more than tens of thousands of images. Moreover, training of a neural network is usually very computationally expensive. One can, however, make use of some already <a href="https://github.com/BVLC/caffe/wiki/Model-Zoo">pretrained models</a> and then just fine-tune them for his particular task.</li>
  <li>We did part of our computations on cluster. As students, our submissions usually needed to wait in queue. Furthermore, once the code is evaluated, one doesn’t have access to variables created during its run. Both these facts make it time consuming to debug one’s code and try various versions of it to see which performs better. After some time, we got into practice of saving some important information during the run (like the mean errors, standard deviations, model parameters, etc..) so that we can more easily track what worked and what not. Moreover, we implemented logging routine that allows to seamlessly track progress of the code evaluation. Here I refer you to <a href="https://docs.python.org/3/howto/logging-cookbook.html">Logging Cookbook</a> and only say that it is extremely useful tool and the extra effort of implementing pays off.</li>
</ul>

<h3 id="conclusion">Conclusion</h3>
<p><br />
So that was it, you should have now an idea what happens behind the scenes when Machine Learning is referred to, what are the most important parts of the learning pipeline, what is it capable of and where some bottlenecks may be. In the scope of this post, I didn’t endeavor to go into details of the mathematical background of respective parts of the pipeline, neither did I discussed how to implement them in code. Although both very interesting, it would be out of scope of an introductory post. If you would like to find out more about the former, than I recommend you to check out some <a href="https://www.quora.com/What-is-the-best-MOOC-to-get-started-in-Machine-Learning">MOOCs</a> or get <a href="https://statweb.stanford.edu/~tibs/ElemStatLearn/">a book</a> that approaches the topic from viewpoint of statistics. If you want to find out more on how are the algorithms implemented in code, than take a look at <a href="http://scikit-learn.org/stable/index.html">scikit-learn documentation</a>. Combination of both is enough to get you started exploring the richness of the machine learning on your own.</p>

<p><br />
<em>Note:
The title image is courtesy of <a href="http://brainposts.blogspot.ch/2010/11/brain-mri-white-matter-intensities.html" target="blank">Brain Posts blog</a>.</em></p>

  </article>

  
<!---
Call Fontawesome in the head section or in the location where you place the share bar
<link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">
--->
<hr>
<br/>
<!---<center><p><script async src="https://eocampaign1.com/form/cfe45fdc-109a-11ef-a9e2-bf0899395051.js" data-form="cfe45fdc-109a-11ef-a9e2-bf0899395051"></script></p></center>--->
<div class="share-box center" style="width: 60%; margin: 0px auto;">
<h2 class="center">Share this:</h2>
<br/>
<a class="l" href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.martinholub.com/2017/06/05/MachineLearningPart2.html&title=Machine Learning - MRI Analysis Project (Part II)&summary=&source=martinholub" onclick="window.open(this.href, 'mywin',
'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;"><i class="fa-brands fa-linkedin"></i><span> linkedin</span></a>

<a class="t" href="https://twitter.com/intent/tweet?text=Machine Learning - MRI Analysis Project (Part II)&url=https://www.martinholub.com/2017/06/05/MachineLearningPart2.html" onclick="window.open(this.href, 'mywin',
'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;"><i class="fa-brands fa-square-x-twitter"></i><span> x</span></a>

<a class="t" href="https://bsky.app/intent/post?text=Machine Learning - MRI Analysis Project (Part II)&url=https://www.martinholub.com/2017/06/05/MachineLearningPart2.html" onclick="window.open(this.href, 'mywin', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;">
  <i class="fa-brands fa-square-bluesky"></i><span> bluesky</span>
</a>
<!---
<a class="g" href="https://plus.google.com/share?url=https://www.martinholub.com/2017/06/05/MachineLearningPart2.html" onclick="window.open(this.href, 'mywin',
'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" ><i class="fa fa-google-plus fa"></i><span> google</span></a>
--->
<a class="r" href="http://www.reddit.com/submit?url=https://www.martinholub.com/2017/06/05/MachineLearningPart2.html" onclick="window.open(this.href, 'mywin',
'left=20,top=20,width=900,height=500,toolbar=1,resizable=0'); return false;"><i class="fa-brands fa-square-reddit"></i><span> reddit</span></a>

<a class="f" href="https://www.facebook.com/sharer/sharer.php?u=https://www.martinholub.com/2017/06/05/MachineLearningPart2.html" onclick="window.open(this.href, 'mywin',
'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" ><i class="fa-brands fa-square-facebook"></i><span> facebook</span></a>

<a class="e" href="mailto:?subject=Machine Learning - MRI Analysis Project (Part II)&amp;body=Check out this site https://www.martinholub.com/2017/06/05/MachineLearningPart2.html"><i class="fa fa-envelope fa"></i><span> email</span></a>
</div>
<br/>
<hr>


  

<div id="disqus_thread"></div>
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'martinholub'; // required: replace example with your forum shortname
        /*var disqus_developer = 1; // Comment out when the site is live */
        var disqus_identifier = "/2017/06/05/MachineLearningPart2.html";

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>



</div>

      </div>
    </div>

<!--
    <span class="contacticon center">
  <a href="https://www.instagram.com/scimartin/" target="_blank"><i class="fa-brands fa-square-instagram"></i></a>
  <a href="https://www.linkedin.com/in/scimartin" target="_blank"><i class="fa-brands fa-linkedin"></i></a>
  <a href="https://bsky.app/profile/martinholub.com" target="_blank"><i class="fa-brands fa-square-bluesky"></i></a>
  <div class="iconsuperwrapper">
  <a href="https://paragraph.xyz/@scimartin" target="_blank">
  <div class="iconwrapper">
  <svg
   class="icon"
   version="1.0"
   width="36"
   height="36"
   viewBox="0 0 36 36"
   preserveAspectRatio="xMidYMid meet"
   id="svg10"
   sodipodi:docname="paragraph.svg"
   inkscape:version="1.1.1 (3bf5ae0d25, 2021-09-20)"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:svg="http://www.w3.org/2000/svg">
  <path
     id="path4"
     d="M 0 0 L 0 12.7273 L 0 25.4546 L 3.73818 25.4546 C 6.04363 25.4546 7.49818 25.4255 7.52727 25.3819 C 7.54909 25.3455 7.61454 25.331 7.66545 25.3528 C 7.82545 25.411 9.96371 25.4254 9.96371 25.3673 C 9.96371 25.3382 10.0073 25.331 10.0582 25.3528 C 10.1164 25.3746 10.2546 25.3964 10.371 25.4037 C 10.5383 25.4182 10.5819 25.4545 10.5746 25.5854 C 10.5746 25.6727 10.5964 25.7455 10.6255 25.7455 C 10.6546 25.7455 10.6546 25.84 10.6183 25.9564 C 10.5892 26.0655 10.5892 26.2036 10.6183 26.2618 C 10.6547 26.3127 10.6547 26.4073 10.6183 26.4728 C 10.5819 26.5382 10.5819 26.6327 10.6109 26.6764 C 10.64 26.7273 10.6473 26.9309 10.6183 27.1273 C 10.5892 27.3237 10.5964 27.5274 10.6255 27.5783 C 10.6546 27.6219 10.6546 27.7164 10.6183 27.7819 C 10.5819 27.8473 10.5819 27.9419 10.6183 28 C 10.6547 28.0582 10.6547 28.1528 10.6183 28.2182 C 10.5819 28.2837 10.5819 28.3782 10.6183 28.4364 C 10.6474 28.4873 10.6474 28.611 10.6183 28.6983 C 10.5747 28.8001 10.5819 28.8873 10.6183 28.9091 C 10.6547 28.9309 10.6547 29.0401 10.6183 29.1637 C 10.5819 29.2873 10.5819 29.3819 10.6183 29.3819 C 10.6547 29.3819 10.6547 29.4764 10.6183 29.6001 C 10.5819 29.7237 10.5819 29.8183 10.6183 29.8183 C 10.6547 29.8183 10.6547 29.9128 10.6183 30.0364 C 10.5819 30.1528 10.5746 30.2546 10.6037 30.2546 C 10.6328 30.2546 10.6546 30.3709 10.6546 30.5091 C 10.6546 30.6473 10.6328 30.7636 10.6037 30.7636 C 10.5746 30.7636 10.5819 30.8582 10.6183 30.9818 C 10.6547 31.0982 10.6547 31.2 10.6183 31.2 C 10.5892 31.2 10.5819 31.2874 10.6109 31.3965 C 10.64 31.5128 10.6401 31.7164 10.6183 31.8691 C 10.5892 32.0146 10.5892 32.1746 10.6255 32.2255 C 10.6546 32.2764 10.6546 32.371 10.6183 32.4364 C 10.5819 32.5019 10.5819 32.5964 10.6183 32.6546 C 10.6547 32.7128 10.6547 32.8073 10.6183 32.8728 C 10.5819 32.9383 10.5819 33.0328 10.6183 33.091 C 10.6474 33.1419 10.6474 33.2655 10.6183 33.3527 C 10.5747 33.4546 10.5819 33.5419 10.6183 33.5637 C 10.6547 33.5855 10.6547 33.6945 10.6183 33.8182 C 10.5819 33.9418 10.5819 34.0364 10.6183 34.0364 C 10.6547 34.0364 10.6547 34.1309 10.6183 34.2545 C 10.5819 34.3782 10.5819 34.4727 10.6183 34.4727 C 10.6547 34.4727 10.6547 34.5673 10.6183 34.6909 C 10.5819 34.8073 10.5746 34.9091 10.6037 34.9091 C 10.6619 34.9091 10.6546 35.4037 10.5965 35.4619 C 10.5747 35.4837 10.582 35.5782 10.6183 35.6727 C 10.6547 35.7673 10.6619 35.8618 10.6401 35.8837 C 10.611 35.9055 16.3128 35.9273 23.2946 35.9273 L 36 35.9273 L 36 22.5455 L 36 9.16364 L 34.7273 9.16364 L 33.4546 9.16364 L 33.4546 21.2873 L 33.4546 33.4183 L 28.5673 33.3964 C 25.8764 33.3891 22.9382 33.3819 22.0364 33.3892 C 21.1345 33.3892 20.3491 33.3964 20.2909 33.3964 C 20.2327 33.3891 18.6327 33.3892 16.7491 33.3892 L 13.3091 33.3818 L 13.3091 29.3819 L 13.3091 25.3746 L 13.5491 25.3818 C 13.6727 25.3818 13.9492 25.3818 14.1601 25.3746 C 14.371 25.3746 14.5746 25.3891 14.6109 25.4109 C 14.6473 25.44 14.6982 25.4255 14.7273 25.3819 C 14.7564 25.3382 14.8074 25.3237 14.8437 25.3528 C 14.9092 25.3891 17.5564 25.4037 18.1091 25.36 C 18.2473 25.3527 18.4364 25.3673 18.5237 25.3964 C 18.611 25.4255 18.7055 25.4182 18.7273 25.3746 C 18.7564 25.3382 18.8073 25.3236 18.8436 25.3527 C 18.8873 25.3745 19.091 25.3963 19.2946 25.3891 C 19.5055 25.3818 19.7455 25.3819 19.8401 25.3819 C 20.1746 25.3819 21.4255 25.1637 21.9273 25.0183 C 22.4364 24.8655 22.56 24.8364 22.7637 24.7855 C 22.8219 24.7782 22.8873 24.7491 22.9091 24.7273 C 22.9309 24.7055 23.2073 24.56 23.5201 24.4073 C 24.1382 24.1164 24.7782 23.7019 25.1127 23.3891 C 25.2218 23.2873 25.3382 23.2001 25.3673 23.1928 C 25.4545 23.1855 26.2037 22.2619 26.5018 21.7964 C 26.8146 21.3164 27.3382 20.0655 27.3455 19.811 C 27.3455 19.731 27.3673 19.6364 27.3964 19.6073 C 27.4255 19.5782 27.4546 19.4327 27.4618 19.28 C 27.4618 19.1273 27.491 18.9601 27.5273 18.9092 C 27.6437 18.7201 27.5928 16.371 27.4546 15.7455 C 27.3891 15.4255 27.3237 15.1127 27.3092 15.0545 C 27.2946 14.96 27.2073 14.7127 27.0764 14.4 C 27.0546 14.3418 27.0255 14.2546 27.0255 14.2182 C 27.0036 14.0582 26.5382 13.2437 26.2255 12.8292 C 25.2219 11.4837 23.3019 10.2764 21.6364 9.92003 C 20.3346 9.64366 20.3564 9.65093 15.4473 9.62184 L 10.6327 9.59275 L 10.6037 9.81093 C 10.5818 9.93457 10.5964 10.0364 10.6255 10.0364 C 10.6546 10.0364 10.6546 10.1309 10.6183 10.2545 C 10.5819 10.3782 10.5819 10.4727 10.6183 10.4727 C 10.6547 10.4727 10.6547 10.5673 10.6183 10.6909 C 10.5819 10.8145 10.5819 10.9091 10.6183 10.9091 C 10.6547 10.9091 10.6547 11.0037 10.6183 11.12 C 10.5892 11.2291 10.5892 11.3673 10.6183 11.4255 C 10.6547 11.4764 10.6547 11.5709 10.6183 11.6364 C 10.5819 11.7019 10.5819 11.7964 10.6183 11.8546 C 10.6547 11.9128 10.6547 12.0073 10.6183 12.0728 C 10.5819 12.1382 10.5819 12.2328 10.6183 12.2909 C 10.6547 12.3491 10.6547 12.4436 10.6183 12.5164 C 10.5819 12.5818 10.5674 12.6546 10.5965 12.6764 C 10.6474 12.7346 10.6546 13.4473 10.5965 13.4982 C 10.5746 13.52 10.5892 13.6364 10.6183 13.7528 C 10.6547 13.8692 10.6547 13.9782 10.6183 14.0001 C 10.5892 14.0219 10.5892 14.1455 10.6183 14.2837 C 10.6474 14.4218 10.6474 14.5745 10.6183 14.6182 C 10.5892 14.6691 10.5892 14.8 10.6183 14.9164 C 10.6547 15.0255 10.6546 15.1345 10.6328 15.1636 C 10.5746 15.2145 10.5965 15.9055 10.6546 16 C 10.6765 16.0437 10.6692 16.0728 10.6256 16.0728 C 10.5819 16.0728 10.5819 16.1528 10.6184 16.291 C 10.6547 16.4146 10.6547 16.5092 10.6184 16.5092 C 10.582 16.5092 10.582 16.6037 10.6184 16.7273 C 10.6547 16.851 10.6547 16.9455 10.6184 16.9455 C 10.582 16.9455 10.582 17.0401 10.6184 17.1637 C 10.6547 17.2873 10.6547 17.3819 10.6184 17.3819 C 10.582 17.3819 10.582 17.4764 10.6184 17.6001 C 10.6547 17.7237 10.6547 17.8182 10.6184 17.8182 C 10.582 17.8182 10.582 17.9128 10.6184 18.0364 C 10.6547 18.1601 10.6547 18.2546 10.6184 18.2546 C 10.582 18.2546 10.582 18.3492 10.6184 18.4728 C 10.6547 18.5964 10.6547 18.691 10.6184 18.691 C 10.582 18.691 10.582 18.7855 10.6184 18.9092 C 10.6547 19.0328 10.6547 19.1273 10.6184 19.1273 C 10.582 19.1273 10.582 19.2219 10.6184 19.3455 C 10.6547 19.4692 10.6547 19.5637 10.6184 19.5637 C 10.582 19.5637 10.5675 19.6073 10.5893 19.6654 C 10.6547 19.84 10.6619 20.24 10.6038 20.4146 C 10.531 20.64 10.6983 20.7419 11.142 20.7346 C 12.0438 20.7128 12.2184 20.72 12.2838 20.7563 C 12.3202 20.7854 12.3711 20.771 12.3929 20.7346 C 12.422 20.6983 12.5238 20.6909 12.6256 20.72 C 12.7347 20.7491 12.931 20.7564 13.0619 20.7419 L 13.3092 20.7128 L 13.2947 16.3855 L 13.2729 12.0582 L 16.6547 12.0946 C 19.9492 12.131 20.5965 12.1745 20.9311 12.3854 C 20.9747 12.4218 21.0183 12.4291 21.0183 12.4073 C 21.0183 12.3346 21.8474 12.6619 22.2474 12.8873 C 23.5492 13.6146 24.2984 14.5819 24.662 16 C 24.8656 16.7782 24.8583 18.2909 24.6546 19.0837 C 24.4582 19.8618 24.0583 20.6546 23.6292 21.1055 C 23.4328 21.3164 23.2729 21.5127 23.2729 21.5418 C 23.2729 21.5709 23.2292 21.6 23.1783 21.6 C 23.1273 21.6 23.0182 21.6582 22.9382 21.7309 C 22.8 21.8473 22.7055 21.9055 22.2255 22.1745 C 21.8764 22.3709 21.1855 22.6182 20.5091 22.7855 C 19.9055 22.931 19.7165 22.9382 11.2219 22.96 L 2.54558 22.9891 L 2.54558 12.7346 L 2.54558 2.47276 L 14.6547 2.47276 L 26.7638 2.47276 L 26.7638 1.23643 L 26.7638 0 L 13.3819 0 Z M 29.3819 0 L 29.3819 1.23643 L 29.3819 2.47276 L 31.4182 2.47276 L 33.4546 2.47276 L 33.4546 4.50916 L 33.4546 6.54545 L 34.7273 6.54545 L 36 6.54545 L 36 3.27273 L 36 0 L 32.6909 0 Z" />
  </svg>
  </div>
  </a>
  </div>
  <br>
  <a href="mailto:scimartin=proton+me"><i class="fa fa-envelope-square"></i></a>
  <a href="https://orcid.org/0000-0002-8365-0927" target="_blank"><i class="ai ai-orcid"></i></a>
  <a href="https://github.com/martinholub" target="_blank"><i class="fa-brands fa-square-github"></i></a>
  <a href="/feed.xml" target="_blank"><i class="fa fa-rss-square"></i></a>
</span>


<div class="col three caption">
	&#169; Martin Holub, 2025
</div>
-->
  <script id="dsq-count-scr" src="//martinholub.disqus.com/count.js" async></script>
  <script src="/css/lightbox.js"></script>

  </body>
    <span class="contacticon center">
  <a href="https://www.instagram.com/scimartin/" target="_blank"><i class="fa-brands fa-square-instagram"></i></a>
  <a href="https://www.linkedin.com/in/scimartin" target="_blank"><i class="fa-brands fa-linkedin"></i></a>
  <a href="https://bsky.app/profile/martinholub.com" target="_blank"><i class="fa-brands fa-square-bluesky"></i></a>
  <div class="iconsuperwrapper">
  <a href="https://paragraph.xyz/@scimartin" target="_blank">
  <div class="iconwrapper">
  <svg
   class="icon"
   version="1.0"
   width="36"
   height="36"
   viewBox="0 0 36 36"
   preserveAspectRatio="xMidYMid meet"
   id="svg10"
   sodipodi:docname="paragraph.svg"
   inkscape:version="1.1.1 (3bf5ae0d25, 2021-09-20)"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:svg="http://www.w3.org/2000/svg">
  <path
     id="path4"
     d="M 0 0 L 0 12.7273 L 0 25.4546 L 3.73818 25.4546 C 6.04363 25.4546 7.49818 25.4255 7.52727 25.3819 C 7.54909 25.3455 7.61454 25.331 7.66545 25.3528 C 7.82545 25.411 9.96371 25.4254 9.96371 25.3673 C 9.96371 25.3382 10.0073 25.331 10.0582 25.3528 C 10.1164 25.3746 10.2546 25.3964 10.371 25.4037 C 10.5383 25.4182 10.5819 25.4545 10.5746 25.5854 C 10.5746 25.6727 10.5964 25.7455 10.6255 25.7455 C 10.6546 25.7455 10.6546 25.84 10.6183 25.9564 C 10.5892 26.0655 10.5892 26.2036 10.6183 26.2618 C 10.6547 26.3127 10.6547 26.4073 10.6183 26.4728 C 10.5819 26.5382 10.5819 26.6327 10.6109 26.6764 C 10.64 26.7273 10.6473 26.9309 10.6183 27.1273 C 10.5892 27.3237 10.5964 27.5274 10.6255 27.5783 C 10.6546 27.6219 10.6546 27.7164 10.6183 27.7819 C 10.5819 27.8473 10.5819 27.9419 10.6183 28 C 10.6547 28.0582 10.6547 28.1528 10.6183 28.2182 C 10.5819 28.2837 10.5819 28.3782 10.6183 28.4364 C 10.6474 28.4873 10.6474 28.611 10.6183 28.6983 C 10.5747 28.8001 10.5819 28.8873 10.6183 28.9091 C 10.6547 28.9309 10.6547 29.0401 10.6183 29.1637 C 10.5819 29.2873 10.5819 29.3819 10.6183 29.3819 C 10.6547 29.3819 10.6547 29.4764 10.6183 29.6001 C 10.5819 29.7237 10.5819 29.8183 10.6183 29.8183 C 10.6547 29.8183 10.6547 29.9128 10.6183 30.0364 C 10.5819 30.1528 10.5746 30.2546 10.6037 30.2546 C 10.6328 30.2546 10.6546 30.3709 10.6546 30.5091 C 10.6546 30.6473 10.6328 30.7636 10.6037 30.7636 C 10.5746 30.7636 10.5819 30.8582 10.6183 30.9818 C 10.6547 31.0982 10.6547 31.2 10.6183 31.2 C 10.5892 31.2 10.5819 31.2874 10.6109 31.3965 C 10.64 31.5128 10.6401 31.7164 10.6183 31.8691 C 10.5892 32.0146 10.5892 32.1746 10.6255 32.2255 C 10.6546 32.2764 10.6546 32.371 10.6183 32.4364 C 10.5819 32.5019 10.5819 32.5964 10.6183 32.6546 C 10.6547 32.7128 10.6547 32.8073 10.6183 32.8728 C 10.5819 32.9383 10.5819 33.0328 10.6183 33.091 C 10.6474 33.1419 10.6474 33.2655 10.6183 33.3527 C 10.5747 33.4546 10.5819 33.5419 10.6183 33.5637 C 10.6547 33.5855 10.6547 33.6945 10.6183 33.8182 C 10.5819 33.9418 10.5819 34.0364 10.6183 34.0364 C 10.6547 34.0364 10.6547 34.1309 10.6183 34.2545 C 10.5819 34.3782 10.5819 34.4727 10.6183 34.4727 C 10.6547 34.4727 10.6547 34.5673 10.6183 34.6909 C 10.5819 34.8073 10.5746 34.9091 10.6037 34.9091 C 10.6619 34.9091 10.6546 35.4037 10.5965 35.4619 C 10.5747 35.4837 10.582 35.5782 10.6183 35.6727 C 10.6547 35.7673 10.6619 35.8618 10.6401 35.8837 C 10.611 35.9055 16.3128 35.9273 23.2946 35.9273 L 36 35.9273 L 36 22.5455 L 36 9.16364 L 34.7273 9.16364 L 33.4546 9.16364 L 33.4546 21.2873 L 33.4546 33.4183 L 28.5673 33.3964 C 25.8764 33.3891 22.9382 33.3819 22.0364 33.3892 C 21.1345 33.3892 20.3491 33.3964 20.2909 33.3964 C 20.2327 33.3891 18.6327 33.3892 16.7491 33.3892 L 13.3091 33.3818 L 13.3091 29.3819 L 13.3091 25.3746 L 13.5491 25.3818 C 13.6727 25.3818 13.9492 25.3818 14.1601 25.3746 C 14.371 25.3746 14.5746 25.3891 14.6109 25.4109 C 14.6473 25.44 14.6982 25.4255 14.7273 25.3819 C 14.7564 25.3382 14.8074 25.3237 14.8437 25.3528 C 14.9092 25.3891 17.5564 25.4037 18.1091 25.36 C 18.2473 25.3527 18.4364 25.3673 18.5237 25.3964 C 18.611 25.4255 18.7055 25.4182 18.7273 25.3746 C 18.7564 25.3382 18.8073 25.3236 18.8436 25.3527 C 18.8873 25.3745 19.091 25.3963 19.2946 25.3891 C 19.5055 25.3818 19.7455 25.3819 19.8401 25.3819 C 20.1746 25.3819 21.4255 25.1637 21.9273 25.0183 C 22.4364 24.8655 22.56 24.8364 22.7637 24.7855 C 22.8219 24.7782 22.8873 24.7491 22.9091 24.7273 C 22.9309 24.7055 23.2073 24.56 23.5201 24.4073 C 24.1382 24.1164 24.7782 23.7019 25.1127 23.3891 C 25.2218 23.2873 25.3382 23.2001 25.3673 23.1928 C 25.4545 23.1855 26.2037 22.2619 26.5018 21.7964 C 26.8146 21.3164 27.3382 20.0655 27.3455 19.811 C 27.3455 19.731 27.3673 19.6364 27.3964 19.6073 C 27.4255 19.5782 27.4546 19.4327 27.4618 19.28 C 27.4618 19.1273 27.491 18.9601 27.5273 18.9092 C 27.6437 18.7201 27.5928 16.371 27.4546 15.7455 C 27.3891 15.4255 27.3237 15.1127 27.3092 15.0545 C 27.2946 14.96 27.2073 14.7127 27.0764 14.4 C 27.0546 14.3418 27.0255 14.2546 27.0255 14.2182 C 27.0036 14.0582 26.5382 13.2437 26.2255 12.8292 C 25.2219 11.4837 23.3019 10.2764 21.6364 9.92003 C 20.3346 9.64366 20.3564 9.65093 15.4473 9.62184 L 10.6327 9.59275 L 10.6037 9.81093 C 10.5818 9.93457 10.5964 10.0364 10.6255 10.0364 C 10.6546 10.0364 10.6546 10.1309 10.6183 10.2545 C 10.5819 10.3782 10.5819 10.4727 10.6183 10.4727 C 10.6547 10.4727 10.6547 10.5673 10.6183 10.6909 C 10.5819 10.8145 10.5819 10.9091 10.6183 10.9091 C 10.6547 10.9091 10.6547 11.0037 10.6183 11.12 C 10.5892 11.2291 10.5892 11.3673 10.6183 11.4255 C 10.6547 11.4764 10.6547 11.5709 10.6183 11.6364 C 10.5819 11.7019 10.5819 11.7964 10.6183 11.8546 C 10.6547 11.9128 10.6547 12.0073 10.6183 12.0728 C 10.5819 12.1382 10.5819 12.2328 10.6183 12.2909 C 10.6547 12.3491 10.6547 12.4436 10.6183 12.5164 C 10.5819 12.5818 10.5674 12.6546 10.5965 12.6764 C 10.6474 12.7346 10.6546 13.4473 10.5965 13.4982 C 10.5746 13.52 10.5892 13.6364 10.6183 13.7528 C 10.6547 13.8692 10.6547 13.9782 10.6183 14.0001 C 10.5892 14.0219 10.5892 14.1455 10.6183 14.2837 C 10.6474 14.4218 10.6474 14.5745 10.6183 14.6182 C 10.5892 14.6691 10.5892 14.8 10.6183 14.9164 C 10.6547 15.0255 10.6546 15.1345 10.6328 15.1636 C 10.5746 15.2145 10.5965 15.9055 10.6546 16 C 10.6765 16.0437 10.6692 16.0728 10.6256 16.0728 C 10.5819 16.0728 10.5819 16.1528 10.6184 16.291 C 10.6547 16.4146 10.6547 16.5092 10.6184 16.5092 C 10.582 16.5092 10.582 16.6037 10.6184 16.7273 C 10.6547 16.851 10.6547 16.9455 10.6184 16.9455 C 10.582 16.9455 10.582 17.0401 10.6184 17.1637 C 10.6547 17.2873 10.6547 17.3819 10.6184 17.3819 C 10.582 17.3819 10.582 17.4764 10.6184 17.6001 C 10.6547 17.7237 10.6547 17.8182 10.6184 17.8182 C 10.582 17.8182 10.582 17.9128 10.6184 18.0364 C 10.6547 18.1601 10.6547 18.2546 10.6184 18.2546 C 10.582 18.2546 10.582 18.3492 10.6184 18.4728 C 10.6547 18.5964 10.6547 18.691 10.6184 18.691 C 10.582 18.691 10.582 18.7855 10.6184 18.9092 C 10.6547 19.0328 10.6547 19.1273 10.6184 19.1273 C 10.582 19.1273 10.582 19.2219 10.6184 19.3455 C 10.6547 19.4692 10.6547 19.5637 10.6184 19.5637 C 10.582 19.5637 10.5675 19.6073 10.5893 19.6654 C 10.6547 19.84 10.6619 20.24 10.6038 20.4146 C 10.531 20.64 10.6983 20.7419 11.142 20.7346 C 12.0438 20.7128 12.2184 20.72 12.2838 20.7563 C 12.3202 20.7854 12.3711 20.771 12.3929 20.7346 C 12.422 20.6983 12.5238 20.6909 12.6256 20.72 C 12.7347 20.7491 12.931 20.7564 13.0619 20.7419 L 13.3092 20.7128 L 13.2947 16.3855 L 13.2729 12.0582 L 16.6547 12.0946 C 19.9492 12.131 20.5965 12.1745 20.9311 12.3854 C 20.9747 12.4218 21.0183 12.4291 21.0183 12.4073 C 21.0183 12.3346 21.8474 12.6619 22.2474 12.8873 C 23.5492 13.6146 24.2984 14.5819 24.662 16 C 24.8656 16.7782 24.8583 18.2909 24.6546 19.0837 C 24.4582 19.8618 24.0583 20.6546 23.6292 21.1055 C 23.4328 21.3164 23.2729 21.5127 23.2729 21.5418 C 23.2729 21.5709 23.2292 21.6 23.1783 21.6 C 23.1273 21.6 23.0182 21.6582 22.9382 21.7309 C 22.8 21.8473 22.7055 21.9055 22.2255 22.1745 C 21.8764 22.3709 21.1855 22.6182 20.5091 22.7855 C 19.9055 22.931 19.7165 22.9382 11.2219 22.96 L 2.54558 22.9891 L 2.54558 12.7346 L 2.54558 2.47276 L 14.6547 2.47276 L 26.7638 2.47276 L 26.7638 1.23643 L 26.7638 0 L 13.3819 0 Z M 29.3819 0 L 29.3819 1.23643 L 29.3819 2.47276 L 31.4182 2.47276 L 33.4546 2.47276 L 33.4546 4.50916 L 33.4546 6.54545 L 34.7273 6.54545 L 36 6.54545 L 36 3.27273 L 36 0 L 32.6909 0 Z" />
  </svg>
  </div>
  </a>
  </div>
  <br>
  <a href="mailto:scimartin=proton+me"><i class="fa fa-envelope-square"></i></a>
  <a href="https://orcid.org/0000-0002-8365-0927" target="_blank"><i class="ai ai-orcid"></i></a>
  <a href="https://github.com/martinholub" target="_blank"><i class="fa-brands fa-square-github"></i></a>
  <a href="/feed.xml" target="_blank"><i class="fa fa-rss-square"></i></a>
</span>


<div class="col three caption">
	&#169; Martin Holub, 2025
</div>

</html>
