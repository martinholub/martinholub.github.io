<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width initial-scale=1" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>Reinforcement Learning, Teaching AI to Play</title>
    <meta name="description" content="A simple, whitespace, helvetica based portfolio theme.
">

    <link rel="stylesheet" href="/css/main.css">
    <link rel="canonical" href="http://localhost:4000/2018/07/14/ReinforcementLearning.html">

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css">
	<link rel="stylesheet" href="/css/academicons.css"/>
    <link href="/css/lightbox.css" rel="stylesheet">

    <link rel="stylesheet" href="/css/zenburn.css"/>

    <!-- collect tags -->
    
      <!-- collect tags from blog posts -->
<!-- source: https://longqian.me/2017/02/09/github-jekyll-tag/ --->







    

	<link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="martin-holub.github.io" />
</head>

  
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-1ZBQKKLMRB"></script>
  <script>
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());

	gtag('config', 'G-1ZBQKKLMRB');
  </script>

  <script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
	</script>

  <body>

    <header class="site-header">

  <div class="wrapper">

    <nav class="site-nav">

      <div class="trigger">
        <!-- martin-holub.github.io instead of blog -->
        <a class="page-link" href="/">about</a>

        <!-- ( for page in site.pages )  was previously-->
        
          
          <a class="page-link" href="/blog">blog</a>
          
        
          
          <a class="page-link" href="/research">research</a>
          
        
          
          <a class="page-link" href="/projects">projects</a>
          
        
          
          <a class="page-link" href="/resources">resources</a>
          
        
          
          <a class="page-link" href="/subscribe">subscribe</a>
          
        

      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Reinforcement Learning, Teaching AI to Play</h1>
    <p class="post-meta">July 14, 2018 — 10:42 • <a href="https://martinholub.github.io/2018/07/14/ReinforcementLearning.html#disqus_thread">0 Comments</a></p>
    <span class="post-meta">[
      
        
          <a href="/tag/ethz"><code class="highligher-rouge"><nobr>ethz</nobr></code>&nbsp;</a>
      
        
          <a href="/tag/projects"><code class="highligher-rouge"><nobr>projects</nobr></code>&nbsp;</a>
      
        
          <a href="/tag/code"><code class="highligher-rouge"><nobr>code</nobr></code>&nbsp;</a>
      
    ]</span>
  </header>
  
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'] ],
      processEscapes: false,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    messageStyle: "normal",
    "HTML-CSS": {
      preferredFont: null
    },
    TeX: {
      extensions: ["AMSmath.js", "AMSsymbols.js","AMScd.js", "autobold.js", "begingroup.js", "mhchem.js"]
    }
  });
</script>
<script
  type="text/javascript"
  charset="utf-8"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
>
</script>


  <article class="post-content">
    <div class="img_row" style="width: 90%;">
  <img class="col three" src="/img/rl/breakout_head.jpg" alt="" title="Header" />
</div>

<p>In this post, I will demonstrate and explain <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">Reinforcement Learning</a> code I developed. You will learn how one can train an AI agent to master Atari games and understand the technology behind <a href="https://deepmind.com/research/alphago/">DeepMind’s AlphaGo</a>, the first computer program to defeat professional human Go player. This will be fun and, surprisingly, simple, so let’s dive in.</p>

<h2 id="openai-gym">OpenAI Gym</h2>

<p><a href="https://github.com/openai/gym">OpenAI’s <code class="language-html highlighter-rouge">gym</code></a> is a toolkit for developing and comparing reinforcement learning algorithms. <a href="https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research">Big standardized datasets</a> have proven pivotal in development of deep learning algorithms. Similarly, collection of test problems, environments, as provided by <code class="language-html highlighter-rouge">gym</code> aids in evaluation of reinforcement learning algorithms when an agent learns to take actions in response to observations from environment to “solve it”. All that comes below is enabled by the work of guys at OpenAI. Hat tip to them.</p>

<h2 id="the-cartpole-problem">The CartPole Problem</h2>

<div class="img_row" style="width: 75%;">
  <img class="col three" src="/img/rl/cartpole.gif" alt="" title="CartPole Agent" />
</div>
<div class="col three caption">
Trained agent skillfully balancing the pole on the cart
</div>

<p>The <code class="language-html highlighter-rouge">CartPole-v0</code> environment is a reinforcement learning (RL) equivalent of <em>Hello World!</em>. As such, it is sufficiently simple to get us started. It is a form of the classic control problem of inverted pendulum where we balance a pole by moving a cart it is attached to either left or right. The environment is considered solved when the average reward is greater than or equal to 195.0 over 100 consecutive trials. You can check the environment’s spec’s at its <a href="https://github.com/openai/gym/wiki/CartPole-v0">wiki</a>.</p>

<h3 id="sarsa">SARSA</h3>

<p>To train an agent for this environment, I will use the <a href="https://en.wikipedia.org/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action">SARSA</a> reinforcement learning algorithm. This is an <em>on-policy</em> learning algorithm (which means that it uses the same policy to generate both current $a_t$ as well as the next action $a_{t+1}$). The name SARSA comes from the fact that the update rule for a <em>Q-value</em> depends on the tuple $(s_t, a_t, r_t, s_{t+1}, a_{t+1})$, where $s_t$, $a_t$ and $r_t$ are state, action and reward at time $t$ respectively.</p>

<p>The concept of <em>Q-value</em> is simple one. You just need to understand that we need a value to optimize that describes the reward the agent has harvested during a period of gameplay. First, define <em>Value function</em> $V^{\pi}(s)$ which is an expected sum of discounted rewards upon starting at state $s$ and selecting actions according to policy $\pi$:</p>

<!-- https://stats.stackexchange.com/questions/326788/when-to-choose-sarsa-vs-q-learning -->
<!-- https://datascience.stackexchange.com/questions/9832/what-is-the-q-function-and-what-is-the-v-function-in-reinforcement-learning/31791#31791 -->

\[V^{\pi}(s) = E[R(s_0)+\gamma R(s_1)+\gamma^2 R(s_2) + ... | s_0=s,\pi].\]

<p>$R(\cdot)$ gives the immediate value of reward and the discount factor $\gamma$ expresses how much we care about past values of it. <em>Q-value</em> function is then just <em>V</em> function for a particular action $a_t$ for each state $s_t$. They are related with:</p>

\[V^\pi(s) = \sum_{a \in A} \pi (a|s)  * Q^\pi(a,s).\]

<p>Given the above definitions, the SARSA algorithm updates the <em>Q-value</em> by an error-term adjusted by the learning rate $\alpha$ as follows:</p>

\[Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha[r_{t+1}+\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t)]\]

<p>In my implementation I <a href="http://artint.info/2e/html/ArtInt2e.Ch12.S9.SS1.html">approximate</a> the <em>Q-values</em> for each action with a linear function and select an action with $argmax(w^Ts)$.</p>

<p>Here is the implementation (full code <a href="https://gist.github.com/martinholub/c4860006d0cf3fbe87a79a054a9c98cd">here</a>):</p>

<script src="https://gist.github.com/c4860006d0cf3fbe87a79a054a9c98cd.js?file=example.py"> </script>

<h3 id="results">Results</h3>

<p>The algorithm works very well and we are able to solve the environment as early as after 60 or so episodes.</p>

<div class="img_row" style="width: 75%;">
  <img class="col three" src="/img/rl/cartpole.png" alt="" title="Cartpole" />
</div>
<div class="col three caption">
Solving the CartPole problem
</div>

<h2 id="atari-games">Atari Games</h2>

<div class="img_row" style="width: 80%;">
  <img class="col three" src="/img/rl/space_invaders.gif" alt="" title="SpaceInvaders" />
</div>
<div class="col three caption">
Example of an Atari Game (Space Invaders)
</div>

<p>After successfully training an agent on a simple problem, I jumped on something more complex. That is, teaching agent how to master <em>Atari games</em>. I focused on the game of <a href="https://gym.openai.com/envs/Breakout-v0/">Breakout</a>. This problem is very different from the previous one in that as input to our model we receive images of the game screen. Learning something from this complex input requires a model that is capable of abstracting features from it. This is why I decided to harness the power of a deep convolutional neural network.</p>

<h3 id="the-deep-model">The Deep Model</h3>

<p>For my implementation, I chose <a href="https://github.com/deepmind/dqn">the DQN model and algorithm</a> that was used by DeepMind to learn how to play Go. I extended it by few recent developments from the field of deep reinforcement learning including the <a href="https://arxiv.org/pdf/1511.06581.pdf">dueling DQN</a> and <a href="https://arxiv.org/pdf/1509.06461.pdf">double DQN</a>. I also used the <a href="https://github.com/rlcode/per">Prioritized Experience Replay Memory</a> to train the agent on samples from past to prevent it forgetting about past experiences. The full code is available at <a href="https://github.com/martinholub/demos-blogs-examples/tree/master/rl-gym/atari">my GitHub</a>.</p>

<p>Here is the model in code:</p>

<script src="https://gist.github.com/083d392d713e515fb5b96379a77670e0.js?file=model.py"> </script>

<p>You will see that the model architecture is pretty big. This is why the researchers at DeepMind had to train it for <em>50 million</em> frames to achieve great results. You will notice that I am using a <a href="https://becominghuman.ai/beat-atari-with-deep-reinforcement-learning-part-2-dqn-improvements-d3563f665a2c">custom loss function</a>. It allows me to update Q-values only for actions that were observed and also avoids the problem of weighting outliers too heavily that otherwise occurs with mean squared loss.</p>

<p>The <a href="https://en.wikipedia.org/wiki/Huber_loss">Huber loss</a> is implemented as follows:</p>

<script src="https://gist.github.com/083d392d713e515fb5b96379a77670e0.js?file=huber_loss.py"> </script>

<h3 id="epsilon-greedy-policy">Epsilon-Greedy Policy</h3>

<p>I used <code class="language-html highlighter-rouge">RMSProp</code> optimizer as it was also the one used in the original paper. Once the memory has been initialized with sufficient number of samples, then the network is retrained on each step by sampling a random batch from memory. The policy implemented is <em>Epsilon Greedy Policy</em> and it differs from the previously described SARSA algorithm in that it is implemented as an <em>off-policy</em> algorithm. That is, it chooses the next action in greedy fashion, in order to maximize the next reward.</p>

<script src="https://gist.github.com/083d392d713e515fb5b96379a77670e0.js?file=training.py"> </script>

<h3 id="checkpointing-and-visualizations">Checkpointing and Visualizations</h3>

<p>Initially, I was getting hardly any improvements, even after many hours of training. I learned the hard way that ample and frequent checkpointing, logging and visualizations are absolute musts in training deep learning models. I found it the most convenient to define these with <a href="https://keras.io/callbacks/">Keras’s <code class="language-html highlighter-rouge">callbacks</code></a> and use <a href="https://www.tensorflow.org/guide/summaries_and_tensorboard"><code class="language-html highlighter-rouge">TensorBoard</code></a> for monitoring them.  Here is an example of callbacks used to update mask of custom loss as well as to add custom visualization to TensorBoard.</p>

<script src="https://gist.github.com/083d392d713e515fb5b96379a77670e0.js?file=callbacks_examples.py"> </script>

<p>For further examples of implementing callbacks in RL, check out the <a href="https://github.com/keras-rl/keras-rl">keras-rl repo</a>.</p>

<div class="img_row">
  <img class="col three" src="/img/rl/tensorboard.png" alt="" title="TensorBoard" />
</div>
<div class="col three caption">
Example of TensorBoard visualizations showing progress on training (on x axis is number of frames).
</div>

<h3 id="results-1">Results</h3>

<p>After about 3000 episodes of training, my agent is able to play the game quite well :)</p>

<div class="img_row" style="width: 50%;">
  <img class="col three" src="/img/rl/breakout.gif" alt="" title="BreakOut" />
</div>
<div class="col three caption">
Independent AI agent playing BreakOut after 3000 episodes of training
</div>

<h2 id="conclusion">Conclusion</h2>

<p>In this post, I have demonstrated that reinforcement learning problems can be solved with reasonably simple, yet very powerful algorithms, I also pin-pointed some critical aspects of training a DL/RL model. The reinforcement learning field has made great leaps forward in past years and it appears to be staying on this track. I am personally really excited about <a href="https://ai.googleblog.com/2018/06/scalable-deep-reinforcement-learning.html">applying RL in robotics</a> and how will AI developed once it receives a physical body with means to experience and explore its environment.</p>

<p><em>Thanks for reading, everyone!</em></p>

<hr />

<h2 id="supplementary-info">Supplementary Info</h2>

<h3 id="takeaways">Takeaways</h3>

<p>Below, I am listing some important personal takeaways. You will benefit from adopting them, especially if you didn’t do much of a practical DL/RL previously.</p>

<ul>
  <li>Make sure you can fit a tiny dataset/simple problem</li>
  <li>Before adopting and adapting reference implementation, test that it works</li>
  <li>Make changes incrementally and test that they produce expected results. First get something that works, only then make it nice.</li>
  <li>For RL, you don’t need neither <a href="https://keras.io/layers/normalization/">BatchNormalization</a> nor <a href="https://keras.io/layers/core/#dropout">Dropout</a>. Regularization is in general much less important than in other settings.</li>
  <li>Visualize loss, metrics, parameter updates, Q-values, actions and more throughout the training (<a href="">Callbacks</a> and <a href="">Tensorboard</a> are your friends)</li>
  <li>Periodically visualize the agent as it plays</li>
  <li>Get proper hardware for training. Either good desktop or remote resources.</li>
</ul>

<p>I also created a <a href="https://github.com/hakimel/reveal.js">Reveal.js</a> presentation where I go through these and other highly relevant and mainly practical aspects of deep learning models and training. <a href="/data/dl-intro/index.html" target="_blank">Check it out here.</a></p>

<h3 id="references">References</h3>

<ul>
  <li><a href="http://artint.info/2e/html/ArtInt2e.Ch12.S9.SS1.html">SARSA with Linear Function Approximation</a></li>
  <li><a href="https://datascience.stackexchange.com/questions/9832/what-is-the-q-function-and-what-is-the-v-function-in-reinforcement-learning/31791#31791">Value and Q-Value</a></li>
  <li><a href="https://stats.stackexchange.com/questions/326788/when-to-choose-sarsa-vs-q-learning">Practical differneces between SARSA and Q-learning</a></li>
</ul>

  </article>

  
<!---
Call Fontawesome in the head section or in the location where you place the share bar
<link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">
--->
<hr>
<br/>
<!---<center><p><script async src="https://eocampaign1.com/form/cfe45fdc-109a-11ef-a9e2-bf0899395051.js" data-form="cfe45fdc-109a-11ef-a9e2-bf0899395051"></script></p></center>--->
<div class="share-box center" style="width: 60%; margin: 0px auto;">
<h2 class="center">Share this:</h2>
<br/>
<a class="l" href="https://www.linkedin.com/shareArticle?mini=true&url=http://localhost:4000/2018/07/14/ReinforcementLearning.html&title=Reinforcement Learning, Teaching AI to Play&summary=&source=martinholub" onclick="window.open(this.href, 'mywin',
'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;"><i class="fa-brands fa-linkedin"></i><span> linkedin</span></a>

<a class="t" href="https://twitter.com/intent/tweet?text=Reinforcement Learning, Teaching AI to Play&url=http://localhost:4000/2018/07/14/ReinforcementLearning.html" onclick="window.open(this.href, 'mywin',
'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;"><i class="fa-brands fa-square-x-twitter"></i><span> x</span></a>

<a class="t" href="https://bsky.app/intent/post?text=Reinforcement Learning, Teaching AI to Play&url=http://localhost:4000/2018/07/14/ReinforcementLearning.html" onclick="window.open(this.href, 'mywin', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;">
  <i class="fa-brands fa-square-bluesky"></i><span> bluesky</span>
</a>
<!---
<a class="g" href="https://plus.google.com/share?url=http://localhost:4000/2018/07/14/ReinforcementLearning.html" onclick="window.open(this.href, 'mywin',
'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" ><i class="fa fa-google-plus fa"></i><span> google</span></a>
--->
<a class="r" href="http://www.reddit.com/submit?url=http://localhost:4000/2018/07/14/ReinforcementLearning.html" onclick="window.open(this.href, 'mywin',
'left=20,top=20,width=900,height=500,toolbar=1,resizable=0'); return false;"><i class="fa-brands fa-square-reddit"></i><span> reddit</span></a>

<a class="f" href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/2018/07/14/ReinforcementLearning.html" onclick="window.open(this.href, 'mywin',
'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" ><i class="fa-brands fa-square-facebook"></i><span> facebook</span></a>

<a class="e" href="mailto:?subject=Reinforcement Learning, Teaching AI to Play&amp;body=Check out this site http://localhost:4000/2018/07/14/ReinforcementLearning.html"><i class="fa fa-envelope fa"></i><span> email</span></a>
</div>
<br/>
<hr>


  

<div id="disqus_thread"></div>
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'martinholub'; // required: replace example with your forum shortname
        /*var disqus_developer = 1; // Comment out when the site is live */
        var disqus_identifier = "/2018/07/14/ReinforcementLearning.html";

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>



</div>

      </div>
    </div>

<!--
    <span class="contacticon center">
  <a href="https://www.instagram.com/scimartin/" target="_blank"><i class="fa-brands fa-square-instagram"></i></a>
  <a href="https://www.linkedin.com/in/scimartin" target="_blank"><i class="fa-brands fa-linkedin"></i></a>
  <a href="https://bsky.app/profile/martinholub.com" target="_blank"><i class="fa-brands fa-square-bluesky"></i></a>
  <div class="iconsuperwrapper">
  <a href="https://paragraph.xyz/@scimartin" target="_blank">
  <div class="iconwrapper">
  <svg
   class="icon"
   version="1.0"
   width="36"
   height="36"
   viewBox="0 0 36 36"
   preserveAspectRatio="xMidYMid meet"
   id="svg10"
   sodipodi:docname="paragraph.svg"
   inkscape:version="1.1.1 (3bf5ae0d25, 2021-09-20)"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:svg="http://www.w3.org/2000/svg">
  <path
     id="path4"
     d="M 0 0 L 0 12.7273 L 0 25.4546 L 3.73818 25.4546 C 6.04363 25.4546 7.49818 25.4255 7.52727 25.3819 C 7.54909 25.3455 7.61454 25.331 7.66545 25.3528 C 7.82545 25.411 9.96371 25.4254 9.96371 25.3673 C 9.96371 25.3382 10.0073 25.331 10.0582 25.3528 C 10.1164 25.3746 10.2546 25.3964 10.371 25.4037 C 10.5383 25.4182 10.5819 25.4545 10.5746 25.5854 C 10.5746 25.6727 10.5964 25.7455 10.6255 25.7455 C 10.6546 25.7455 10.6546 25.84 10.6183 25.9564 C 10.5892 26.0655 10.5892 26.2036 10.6183 26.2618 C 10.6547 26.3127 10.6547 26.4073 10.6183 26.4728 C 10.5819 26.5382 10.5819 26.6327 10.6109 26.6764 C 10.64 26.7273 10.6473 26.9309 10.6183 27.1273 C 10.5892 27.3237 10.5964 27.5274 10.6255 27.5783 C 10.6546 27.6219 10.6546 27.7164 10.6183 27.7819 C 10.5819 27.8473 10.5819 27.9419 10.6183 28 C 10.6547 28.0582 10.6547 28.1528 10.6183 28.2182 C 10.5819 28.2837 10.5819 28.3782 10.6183 28.4364 C 10.6474 28.4873 10.6474 28.611 10.6183 28.6983 C 10.5747 28.8001 10.5819 28.8873 10.6183 28.9091 C 10.6547 28.9309 10.6547 29.0401 10.6183 29.1637 C 10.5819 29.2873 10.5819 29.3819 10.6183 29.3819 C 10.6547 29.3819 10.6547 29.4764 10.6183 29.6001 C 10.5819 29.7237 10.5819 29.8183 10.6183 29.8183 C 10.6547 29.8183 10.6547 29.9128 10.6183 30.0364 C 10.5819 30.1528 10.5746 30.2546 10.6037 30.2546 C 10.6328 30.2546 10.6546 30.3709 10.6546 30.5091 C 10.6546 30.6473 10.6328 30.7636 10.6037 30.7636 C 10.5746 30.7636 10.5819 30.8582 10.6183 30.9818 C 10.6547 31.0982 10.6547 31.2 10.6183 31.2 C 10.5892 31.2 10.5819 31.2874 10.6109 31.3965 C 10.64 31.5128 10.6401 31.7164 10.6183 31.8691 C 10.5892 32.0146 10.5892 32.1746 10.6255 32.2255 C 10.6546 32.2764 10.6546 32.371 10.6183 32.4364 C 10.5819 32.5019 10.5819 32.5964 10.6183 32.6546 C 10.6547 32.7128 10.6547 32.8073 10.6183 32.8728 C 10.5819 32.9383 10.5819 33.0328 10.6183 33.091 C 10.6474 33.1419 10.6474 33.2655 10.6183 33.3527 C 10.5747 33.4546 10.5819 33.5419 10.6183 33.5637 C 10.6547 33.5855 10.6547 33.6945 10.6183 33.8182 C 10.5819 33.9418 10.5819 34.0364 10.6183 34.0364 C 10.6547 34.0364 10.6547 34.1309 10.6183 34.2545 C 10.5819 34.3782 10.5819 34.4727 10.6183 34.4727 C 10.6547 34.4727 10.6547 34.5673 10.6183 34.6909 C 10.5819 34.8073 10.5746 34.9091 10.6037 34.9091 C 10.6619 34.9091 10.6546 35.4037 10.5965 35.4619 C 10.5747 35.4837 10.582 35.5782 10.6183 35.6727 C 10.6547 35.7673 10.6619 35.8618 10.6401 35.8837 C 10.611 35.9055 16.3128 35.9273 23.2946 35.9273 L 36 35.9273 L 36 22.5455 L 36 9.16364 L 34.7273 9.16364 L 33.4546 9.16364 L 33.4546 21.2873 L 33.4546 33.4183 L 28.5673 33.3964 C 25.8764 33.3891 22.9382 33.3819 22.0364 33.3892 C 21.1345 33.3892 20.3491 33.3964 20.2909 33.3964 C 20.2327 33.3891 18.6327 33.3892 16.7491 33.3892 L 13.3091 33.3818 L 13.3091 29.3819 L 13.3091 25.3746 L 13.5491 25.3818 C 13.6727 25.3818 13.9492 25.3818 14.1601 25.3746 C 14.371 25.3746 14.5746 25.3891 14.6109 25.4109 C 14.6473 25.44 14.6982 25.4255 14.7273 25.3819 C 14.7564 25.3382 14.8074 25.3237 14.8437 25.3528 C 14.9092 25.3891 17.5564 25.4037 18.1091 25.36 C 18.2473 25.3527 18.4364 25.3673 18.5237 25.3964 C 18.611 25.4255 18.7055 25.4182 18.7273 25.3746 C 18.7564 25.3382 18.8073 25.3236 18.8436 25.3527 C 18.8873 25.3745 19.091 25.3963 19.2946 25.3891 C 19.5055 25.3818 19.7455 25.3819 19.8401 25.3819 C 20.1746 25.3819 21.4255 25.1637 21.9273 25.0183 C 22.4364 24.8655 22.56 24.8364 22.7637 24.7855 C 22.8219 24.7782 22.8873 24.7491 22.9091 24.7273 C 22.9309 24.7055 23.2073 24.56 23.5201 24.4073 C 24.1382 24.1164 24.7782 23.7019 25.1127 23.3891 C 25.2218 23.2873 25.3382 23.2001 25.3673 23.1928 C 25.4545 23.1855 26.2037 22.2619 26.5018 21.7964 C 26.8146 21.3164 27.3382 20.0655 27.3455 19.811 C 27.3455 19.731 27.3673 19.6364 27.3964 19.6073 C 27.4255 19.5782 27.4546 19.4327 27.4618 19.28 C 27.4618 19.1273 27.491 18.9601 27.5273 18.9092 C 27.6437 18.7201 27.5928 16.371 27.4546 15.7455 C 27.3891 15.4255 27.3237 15.1127 27.3092 15.0545 C 27.2946 14.96 27.2073 14.7127 27.0764 14.4 C 27.0546 14.3418 27.0255 14.2546 27.0255 14.2182 C 27.0036 14.0582 26.5382 13.2437 26.2255 12.8292 C 25.2219 11.4837 23.3019 10.2764 21.6364 9.92003 C 20.3346 9.64366 20.3564 9.65093 15.4473 9.62184 L 10.6327 9.59275 L 10.6037 9.81093 C 10.5818 9.93457 10.5964 10.0364 10.6255 10.0364 C 10.6546 10.0364 10.6546 10.1309 10.6183 10.2545 C 10.5819 10.3782 10.5819 10.4727 10.6183 10.4727 C 10.6547 10.4727 10.6547 10.5673 10.6183 10.6909 C 10.5819 10.8145 10.5819 10.9091 10.6183 10.9091 C 10.6547 10.9091 10.6547 11.0037 10.6183 11.12 C 10.5892 11.2291 10.5892 11.3673 10.6183 11.4255 C 10.6547 11.4764 10.6547 11.5709 10.6183 11.6364 C 10.5819 11.7019 10.5819 11.7964 10.6183 11.8546 C 10.6547 11.9128 10.6547 12.0073 10.6183 12.0728 C 10.5819 12.1382 10.5819 12.2328 10.6183 12.2909 C 10.6547 12.3491 10.6547 12.4436 10.6183 12.5164 C 10.5819 12.5818 10.5674 12.6546 10.5965 12.6764 C 10.6474 12.7346 10.6546 13.4473 10.5965 13.4982 C 10.5746 13.52 10.5892 13.6364 10.6183 13.7528 C 10.6547 13.8692 10.6547 13.9782 10.6183 14.0001 C 10.5892 14.0219 10.5892 14.1455 10.6183 14.2837 C 10.6474 14.4218 10.6474 14.5745 10.6183 14.6182 C 10.5892 14.6691 10.5892 14.8 10.6183 14.9164 C 10.6547 15.0255 10.6546 15.1345 10.6328 15.1636 C 10.5746 15.2145 10.5965 15.9055 10.6546 16 C 10.6765 16.0437 10.6692 16.0728 10.6256 16.0728 C 10.5819 16.0728 10.5819 16.1528 10.6184 16.291 C 10.6547 16.4146 10.6547 16.5092 10.6184 16.5092 C 10.582 16.5092 10.582 16.6037 10.6184 16.7273 C 10.6547 16.851 10.6547 16.9455 10.6184 16.9455 C 10.582 16.9455 10.582 17.0401 10.6184 17.1637 C 10.6547 17.2873 10.6547 17.3819 10.6184 17.3819 C 10.582 17.3819 10.582 17.4764 10.6184 17.6001 C 10.6547 17.7237 10.6547 17.8182 10.6184 17.8182 C 10.582 17.8182 10.582 17.9128 10.6184 18.0364 C 10.6547 18.1601 10.6547 18.2546 10.6184 18.2546 C 10.582 18.2546 10.582 18.3492 10.6184 18.4728 C 10.6547 18.5964 10.6547 18.691 10.6184 18.691 C 10.582 18.691 10.582 18.7855 10.6184 18.9092 C 10.6547 19.0328 10.6547 19.1273 10.6184 19.1273 C 10.582 19.1273 10.582 19.2219 10.6184 19.3455 C 10.6547 19.4692 10.6547 19.5637 10.6184 19.5637 C 10.582 19.5637 10.5675 19.6073 10.5893 19.6654 C 10.6547 19.84 10.6619 20.24 10.6038 20.4146 C 10.531 20.64 10.6983 20.7419 11.142 20.7346 C 12.0438 20.7128 12.2184 20.72 12.2838 20.7563 C 12.3202 20.7854 12.3711 20.771 12.3929 20.7346 C 12.422 20.6983 12.5238 20.6909 12.6256 20.72 C 12.7347 20.7491 12.931 20.7564 13.0619 20.7419 L 13.3092 20.7128 L 13.2947 16.3855 L 13.2729 12.0582 L 16.6547 12.0946 C 19.9492 12.131 20.5965 12.1745 20.9311 12.3854 C 20.9747 12.4218 21.0183 12.4291 21.0183 12.4073 C 21.0183 12.3346 21.8474 12.6619 22.2474 12.8873 C 23.5492 13.6146 24.2984 14.5819 24.662 16 C 24.8656 16.7782 24.8583 18.2909 24.6546 19.0837 C 24.4582 19.8618 24.0583 20.6546 23.6292 21.1055 C 23.4328 21.3164 23.2729 21.5127 23.2729 21.5418 C 23.2729 21.5709 23.2292 21.6 23.1783 21.6 C 23.1273 21.6 23.0182 21.6582 22.9382 21.7309 C 22.8 21.8473 22.7055 21.9055 22.2255 22.1745 C 21.8764 22.3709 21.1855 22.6182 20.5091 22.7855 C 19.9055 22.931 19.7165 22.9382 11.2219 22.96 L 2.54558 22.9891 L 2.54558 12.7346 L 2.54558 2.47276 L 14.6547 2.47276 L 26.7638 2.47276 L 26.7638 1.23643 L 26.7638 0 L 13.3819 0 Z M 29.3819 0 L 29.3819 1.23643 L 29.3819 2.47276 L 31.4182 2.47276 L 33.4546 2.47276 L 33.4546 4.50916 L 33.4546 6.54545 L 34.7273 6.54545 L 36 6.54545 L 36 3.27273 L 36 0 L 32.6909 0 Z" />
  </svg>
  </div>
  </a>
  </div>
  <br>
  <a href="mailto:scimartin=proton+me"><i class="fa fa-envelope-square"></i></a>
  <a href="https://orcid.org/0000-0002-8365-0927" target="_blank"><i class="ai ai-orcid"></i></a>
  <a href="https://github.com/martinholub" target="_blank"><i class="fa-brands fa-square-github"></i></a>
  <a href="/feed.xml" target="_blank"><i class="fa fa-rss-square"></i></a>
</span>


<div class="col three caption">
	&#169; Martin Holub, 2025
</div>
-->
  <script id="dsq-count-scr" src="//martinholub.disqus.com/count.js" async></script>
  <script src="/css/lightbox.js"></script>

  </body>
    <span class="contacticon center">
  <a href="https://www.instagram.com/scimartin/" target="_blank"><i class="fa-brands fa-square-instagram"></i></a>
  <a href="https://www.linkedin.com/in/scimartin" target="_blank"><i class="fa-brands fa-linkedin"></i></a>
  <a href="https://bsky.app/profile/martinholub.com" target="_blank"><i class="fa-brands fa-square-bluesky"></i></a>
  <div class="iconsuperwrapper">
  <a href="https://paragraph.xyz/@scimartin" target="_blank">
  <div class="iconwrapper">
  <svg
   class="icon"
   version="1.0"
   width="36"
   height="36"
   viewBox="0 0 36 36"
   preserveAspectRatio="xMidYMid meet"
   id="svg10"
   sodipodi:docname="paragraph.svg"
   inkscape:version="1.1.1 (3bf5ae0d25, 2021-09-20)"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:svg="http://www.w3.org/2000/svg">
  <path
     id="path4"
     d="M 0 0 L 0 12.7273 L 0 25.4546 L 3.73818 25.4546 C 6.04363 25.4546 7.49818 25.4255 7.52727 25.3819 C 7.54909 25.3455 7.61454 25.331 7.66545 25.3528 C 7.82545 25.411 9.96371 25.4254 9.96371 25.3673 C 9.96371 25.3382 10.0073 25.331 10.0582 25.3528 C 10.1164 25.3746 10.2546 25.3964 10.371 25.4037 C 10.5383 25.4182 10.5819 25.4545 10.5746 25.5854 C 10.5746 25.6727 10.5964 25.7455 10.6255 25.7455 C 10.6546 25.7455 10.6546 25.84 10.6183 25.9564 C 10.5892 26.0655 10.5892 26.2036 10.6183 26.2618 C 10.6547 26.3127 10.6547 26.4073 10.6183 26.4728 C 10.5819 26.5382 10.5819 26.6327 10.6109 26.6764 C 10.64 26.7273 10.6473 26.9309 10.6183 27.1273 C 10.5892 27.3237 10.5964 27.5274 10.6255 27.5783 C 10.6546 27.6219 10.6546 27.7164 10.6183 27.7819 C 10.5819 27.8473 10.5819 27.9419 10.6183 28 C 10.6547 28.0582 10.6547 28.1528 10.6183 28.2182 C 10.5819 28.2837 10.5819 28.3782 10.6183 28.4364 C 10.6474 28.4873 10.6474 28.611 10.6183 28.6983 C 10.5747 28.8001 10.5819 28.8873 10.6183 28.9091 C 10.6547 28.9309 10.6547 29.0401 10.6183 29.1637 C 10.5819 29.2873 10.5819 29.3819 10.6183 29.3819 C 10.6547 29.3819 10.6547 29.4764 10.6183 29.6001 C 10.5819 29.7237 10.5819 29.8183 10.6183 29.8183 C 10.6547 29.8183 10.6547 29.9128 10.6183 30.0364 C 10.5819 30.1528 10.5746 30.2546 10.6037 30.2546 C 10.6328 30.2546 10.6546 30.3709 10.6546 30.5091 C 10.6546 30.6473 10.6328 30.7636 10.6037 30.7636 C 10.5746 30.7636 10.5819 30.8582 10.6183 30.9818 C 10.6547 31.0982 10.6547 31.2 10.6183 31.2 C 10.5892 31.2 10.5819 31.2874 10.6109 31.3965 C 10.64 31.5128 10.6401 31.7164 10.6183 31.8691 C 10.5892 32.0146 10.5892 32.1746 10.6255 32.2255 C 10.6546 32.2764 10.6546 32.371 10.6183 32.4364 C 10.5819 32.5019 10.5819 32.5964 10.6183 32.6546 C 10.6547 32.7128 10.6547 32.8073 10.6183 32.8728 C 10.5819 32.9383 10.5819 33.0328 10.6183 33.091 C 10.6474 33.1419 10.6474 33.2655 10.6183 33.3527 C 10.5747 33.4546 10.5819 33.5419 10.6183 33.5637 C 10.6547 33.5855 10.6547 33.6945 10.6183 33.8182 C 10.5819 33.9418 10.5819 34.0364 10.6183 34.0364 C 10.6547 34.0364 10.6547 34.1309 10.6183 34.2545 C 10.5819 34.3782 10.5819 34.4727 10.6183 34.4727 C 10.6547 34.4727 10.6547 34.5673 10.6183 34.6909 C 10.5819 34.8073 10.5746 34.9091 10.6037 34.9091 C 10.6619 34.9091 10.6546 35.4037 10.5965 35.4619 C 10.5747 35.4837 10.582 35.5782 10.6183 35.6727 C 10.6547 35.7673 10.6619 35.8618 10.6401 35.8837 C 10.611 35.9055 16.3128 35.9273 23.2946 35.9273 L 36 35.9273 L 36 22.5455 L 36 9.16364 L 34.7273 9.16364 L 33.4546 9.16364 L 33.4546 21.2873 L 33.4546 33.4183 L 28.5673 33.3964 C 25.8764 33.3891 22.9382 33.3819 22.0364 33.3892 C 21.1345 33.3892 20.3491 33.3964 20.2909 33.3964 C 20.2327 33.3891 18.6327 33.3892 16.7491 33.3892 L 13.3091 33.3818 L 13.3091 29.3819 L 13.3091 25.3746 L 13.5491 25.3818 C 13.6727 25.3818 13.9492 25.3818 14.1601 25.3746 C 14.371 25.3746 14.5746 25.3891 14.6109 25.4109 C 14.6473 25.44 14.6982 25.4255 14.7273 25.3819 C 14.7564 25.3382 14.8074 25.3237 14.8437 25.3528 C 14.9092 25.3891 17.5564 25.4037 18.1091 25.36 C 18.2473 25.3527 18.4364 25.3673 18.5237 25.3964 C 18.611 25.4255 18.7055 25.4182 18.7273 25.3746 C 18.7564 25.3382 18.8073 25.3236 18.8436 25.3527 C 18.8873 25.3745 19.091 25.3963 19.2946 25.3891 C 19.5055 25.3818 19.7455 25.3819 19.8401 25.3819 C 20.1746 25.3819 21.4255 25.1637 21.9273 25.0183 C 22.4364 24.8655 22.56 24.8364 22.7637 24.7855 C 22.8219 24.7782 22.8873 24.7491 22.9091 24.7273 C 22.9309 24.7055 23.2073 24.56 23.5201 24.4073 C 24.1382 24.1164 24.7782 23.7019 25.1127 23.3891 C 25.2218 23.2873 25.3382 23.2001 25.3673 23.1928 C 25.4545 23.1855 26.2037 22.2619 26.5018 21.7964 C 26.8146 21.3164 27.3382 20.0655 27.3455 19.811 C 27.3455 19.731 27.3673 19.6364 27.3964 19.6073 C 27.4255 19.5782 27.4546 19.4327 27.4618 19.28 C 27.4618 19.1273 27.491 18.9601 27.5273 18.9092 C 27.6437 18.7201 27.5928 16.371 27.4546 15.7455 C 27.3891 15.4255 27.3237 15.1127 27.3092 15.0545 C 27.2946 14.96 27.2073 14.7127 27.0764 14.4 C 27.0546 14.3418 27.0255 14.2546 27.0255 14.2182 C 27.0036 14.0582 26.5382 13.2437 26.2255 12.8292 C 25.2219 11.4837 23.3019 10.2764 21.6364 9.92003 C 20.3346 9.64366 20.3564 9.65093 15.4473 9.62184 L 10.6327 9.59275 L 10.6037 9.81093 C 10.5818 9.93457 10.5964 10.0364 10.6255 10.0364 C 10.6546 10.0364 10.6546 10.1309 10.6183 10.2545 C 10.5819 10.3782 10.5819 10.4727 10.6183 10.4727 C 10.6547 10.4727 10.6547 10.5673 10.6183 10.6909 C 10.5819 10.8145 10.5819 10.9091 10.6183 10.9091 C 10.6547 10.9091 10.6547 11.0037 10.6183 11.12 C 10.5892 11.2291 10.5892 11.3673 10.6183 11.4255 C 10.6547 11.4764 10.6547 11.5709 10.6183 11.6364 C 10.5819 11.7019 10.5819 11.7964 10.6183 11.8546 C 10.6547 11.9128 10.6547 12.0073 10.6183 12.0728 C 10.5819 12.1382 10.5819 12.2328 10.6183 12.2909 C 10.6547 12.3491 10.6547 12.4436 10.6183 12.5164 C 10.5819 12.5818 10.5674 12.6546 10.5965 12.6764 C 10.6474 12.7346 10.6546 13.4473 10.5965 13.4982 C 10.5746 13.52 10.5892 13.6364 10.6183 13.7528 C 10.6547 13.8692 10.6547 13.9782 10.6183 14.0001 C 10.5892 14.0219 10.5892 14.1455 10.6183 14.2837 C 10.6474 14.4218 10.6474 14.5745 10.6183 14.6182 C 10.5892 14.6691 10.5892 14.8 10.6183 14.9164 C 10.6547 15.0255 10.6546 15.1345 10.6328 15.1636 C 10.5746 15.2145 10.5965 15.9055 10.6546 16 C 10.6765 16.0437 10.6692 16.0728 10.6256 16.0728 C 10.5819 16.0728 10.5819 16.1528 10.6184 16.291 C 10.6547 16.4146 10.6547 16.5092 10.6184 16.5092 C 10.582 16.5092 10.582 16.6037 10.6184 16.7273 C 10.6547 16.851 10.6547 16.9455 10.6184 16.9455 C 10.582 16.9455 10.582 17.0401 10.6184 17.1637 C 10.6547 17.2873 10.6547 17.3819 10.6184 17.3819 C 10.582 17.3819 10.582 17.4764 10.6184 17.6001 C 10.6547 17.7237 10.6547 17.8182 10.6184 17.8182 C 10.582 17.8182 10.582 17.9128 10.6184 18.0364 C 10.6547 18.1601 10.6547 18.2546 10.6184 18.2546 C 10.582 18.2546 10.582 18.3492 10.6184 18.4728 C 10.6547 18.5964 10.6547 18.691 10.6184 18.691 C 10.582 18.691 10.582 18.7855 10.6184 18.9092 C 10.6547 19.0328 10.6547 19.1273 10.6184 19.1273 C 10.582 19.1273 10.582 19.2219 10.6184 19.3455 C 10.6547 19.4692 10.6547 19.5637 10.6184 19.5637 C 10.582 19.5637 10.5675 19.6073 10.5893 19.6654 C 10.6547 19.84 10.6619 20.24 10.6038 20.4146 C 10.531 20.64 10.6983 20.7419 11.142 20.7346 C 12.0438 20.7128 12.2184 20.72 12.2838 20.7563 C 12.3202 20.7854 12.3711 20.771 12.3929 20.7346 C 12.422 20.6983 12.5238 20.6909 12.6256 20.72 C 12.7347 20.7491 12.931 20.7564 13.0619 20.7419 L 13.3092 20.7128 L 13.2947 16.3855 L 13.2729 12.0582 L 16.6547 12.0946 C 19.9492 12.131 20.5965 12.1745 20.9311 12.3854 C 20.9747 12.4218 21.0183 12.4291 21.0183 12.4073 C 21.0183 12.3346 21.8474 12.6619 22.2474 12.8873 C 23.5492 13.6146 24.2984 14.5819 24.662 16 C 24.8656 16.7782 24.8583 18.2909 24.6546 19.0837 C 24.4582 19.8618 24.0583 20.6546 23.6292 21.1055 C 23.4328 21.3164 23.2729 21.5127 23.2729 21.5418 C 23.2729 21.5709 23.2292 21.6 23.1783 21.6 C 23.1273 21.6 23.0182 21.6582 22.9382 21.7309 C 22.8 21.8473 22.7055 21.9055 22.2255 22.1745 C 21.8764 22.3709 21.1855 22.6182 20.5091 22.7855 C 19.9055 22.931 19.7165 22.9382 11.2219 22.96 L 2.54558 22.9891 L 2.54558 12.7346 L 2.54558 2.47276 L 14.6547 2.47276 L 26.7638 2.47276 L 26.7638 1.23643 L 26.7638 0 L 13.3819 0 Z M 29.3819 0 L 29.3819 1.23643 L 29.3819 2.47276 L 31.4182 2.47276 L 33.4546 2.47276 L 33.4546 4.50916 L 33.4546 6.54545 L 34.7273 6.54545 L 36 6.54545 L 36 3.27273 L 36 0 L 32.6909 0 Z" />
  </svg>
  </div>
  </a>
  </div>
  <br>
  <a href="mailto:scimartin=proton+me"><i class="fa fa-envelope-square"></i></a>
  <a href="https://orcid.org/0000-0002-8365-0927" target="_blank"><i class="ai ai-orcid"></i></a>
  <a href="https://github.com/martinholub" target="_blank"><i class="fa-brands fa-square-github"></i></a>
  <a href="/feed.xml" target="_blank"><i class="fa fa-rss-square"></i></a>
</span>


<div class="col three caption">
	&#169; Martin Holub, 2025
</div>

</html>
