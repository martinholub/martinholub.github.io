{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Everything You Always Wanted to Know About\n",
    "# Deep Learning\n",
    "\n",
    "---\n",
    "\n",
    "Martin Holub, 05/07/2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<!--![](assets/say-deep-learning-one-more-time.jpg)-->\n",
    "<img src=\"assets/say-deep-learning-one-more-time.jpg\" style=\"width: 350px;\" align=\"center;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "[how to slides](https://medium.com/@mjspeck/presenting-code-using-jupyter-notebook-slides-a8a3c3b59d67)\n",
    "[how to slides 2](https://medium.com/learning-machine-learning/present-your-data-science-projects-with-jupyter-slides-75f20735eb0f)\n",
    "\n",
    "**Run me with:** \n",
    "```\n",
    "jupyter nbconvert dl-intro-slides.ipynb --to slides --post serve --reveal-prefix http://cdn.rawgit.com/hakimel/reveal.js/3.5.0/\n",
    "```\n",
    "\n",
    "In this presentation I will talk about Deep Learning. I will assume some previous knowledge on your part, but will still quickly refresh some basic concepts.\n",
    "\n",
    "I am no expert on the field. It is wast and moving quickly. Moreover we have just about an hour for the presentation. Despite that, I believe that it encapsulates the most important aspects and you shall benefit from it. \n",
    "\n",
    "My main focus is on **practical aspects.** Here I clearly see a value of just hearing about them as compared to understanding the concepts, which is also immensely useful, but clearly beyond the time we have here.\n",
    "\n",
    "You will learn about:\n",
    "* How does DL differ from ML\n",
    "* Why is DL so succesfull after getting out of favor decades ago?\n",
    "* Basics of Machine Learning:\n",
    "  * features, regularization, overfitting\n",
    "  * bias/variance\n",
    "  * linear regression\n",
    "  * kernel engineering\n",
    "\n",
    "** Ask!, Ask!, Ask! **\n",
    "\n",
    "Also: Print the google document on applying ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"assets/input_asses.png\" style=\"width: 500px;\" align=\"center;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Landscape of AI\n",
    "\n",
    "[]() | []()\n",
    ":---:|:---:\n",
    "![](assets/ai_landscape.png)| ![](assets/ai_flowchart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **Artificial Intelligence**: The study of *intelligent agents*, systems that perceive their environment and take actions that maximize their chance of successfully achieving their goals\n",
    "\n",
    "* **Machine Learning**: A computer program is said to *learn from experience* E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E.\n",
    "\n",
    "* **Representation Learning** (Feature Learning): Set of techniques that allows a system to automatically *discover the representations* needed for learning.\n",
    "\n",
    "* **Deep Learning** (Hierarchical Learning): Class of machine learning algorithms that learn *multiple levels of representations* that correspond to *different levels of abstraction*; the levels form a hierarchy of concepts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In early days of *artificial intelligence*, the field reaped successes on problems that can be described by set of formal mathematical rules. These rules were usually hard-coded.\n",
    "\n",
    "This of course was significant limitation and it was necessary for the AI systems to extract patterns from raw data. The field of *machine learning* was born. In classical machine learning, we decide what will be the input to the algorithm. I.e. we engineer the features. Consider an example of ceaserian delivery. The AI system can recommend this procedure, but it does not do so by examining the patient directly, instead it is fed by several pieces of relevant information about the patient, usually designed by the doctor. Each peace of this information is called *featrue* and it is an *representation* of the patient. The machine learning algorithm is heavily dependent on the choice of representations. As an example from everyday life, compare doing arithmetics on Arabic numbers versus Roman numbers. Clearly one representation lends itself much better for this task. Many AI tasks can be solved by engineering the right set of features.\n",
    "\n",
    "However, for many tasks it is difficult to know which features should be extracted. One solution to this problem is to use machine learning not only to discover the mapping from features to output but also the representations themselves. Enter the world of *representation learning*.\n",
    "\n",
    "Whereas some features are extracted readily, others, usually high-level abstract features become very difficult to disentagle from the raw input data.\n",
    "\n",
    "*Deep Learning* addresses this by stacking layers on neurons and henceforth  constructing features in hierichical fashion from simpler features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Features\n",
    " \n",
    "[]() | []()\n",
    ":---:|:---:\n",
    "![Example of different representations](assets/features1.png) | ![Building abstract features](assets/representations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Expert features < deep features\n",
    "<img src=\"assets/sift_paper.png\" style=\"width: 700px;\"/>\n",
    "<img src=\"assets/imagenet.png\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In past, features were engineered by domain experts. This worked quite well, but it took whole community tens of years before they came up with some that gave decent results. The features were often highly domain specific and not easily transferable.\n",
    "\n",
    "On the other hand, neural nets have managed to learn features that enabled the models to signficantly surpass the performance of previous approaches.\n",
    "\n",
    "The features are beyond what humans could come up with, make sense when visualized and are transferable between domains. Domain adaptation does not require domain specific-knowledge (some DL softEng skill needed though)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Mo' Data, Mo' GPUs\n",
    "\n",
    "> It is true that some skill is required to get good performance from a deep learning algorithm. Fortunately, the amount of skill required reduces as the amount of training data increases. The learning algorithms reaching human performance on complex tasks today are nearly identical to the learning algorithms that struggled to solve toy problems in the 1980s [...] The **most important new development is that today we can provide these algorithms with the resources they need to succeed**.\n",
    "\n",
    "> Another key reason that neural networks are wildly successful today after enjoying comparatively little success since the 1980s is that we have the computational resources to run **much larger models today**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "More computaitonal pwoer, allows us to train bigger models on bigger datasets. This in turns allows for more abstract and complex feeatures to be learned. It is easier to construct such features in hiararchical fashion, i.e. by more depth.\n",
    "\n",
    "Apart from this, it is worth noting that couple of engineering tricks make the training easier. This includes replacing mean squared error with cross-entropy loss or using ReLUs as activation units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<!--![](assets/its_the_data.jpg)-->\n",
    "<img src=\"assets/its_the_data.jpg\" style=\"width: 250px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Bigger Models | More Data\n",
    ":---: | :---:\n",
    "<img src=\"assets/network_size.png\" style=\"width: 400px;\"/> | <img src=\"assets/dataset_size.png\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* As of 2016, the rule of thumb is that supervised deep learning algorithm will generally achieve **acceptable performance with around 5000 examples per category** and will **match or exceed human performance** when trained on dataset with at leas **10 million labeled examples**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"assets/ai_flowchart.png\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We start our journey with brief introduction to Machine Learning. Recall that DL differs mainly in how we obtain features, else the challenges and critical aspects are very alike.\n",
    "\n",
    "I begin with definition of ML. To make it more concrete, I immediatelly follow up with an example.\n",
    "\n",
    "I mostly talk about supervised learning, as unsupervised is more rare. Unsupervised is usually sth along the lines of clustering (k Means), dimensionality reduction (PCA, ICA, Autoencoders), or generative models (GANs, Deep belief Nets). Semisupervised Learning is attractive is limited amount of labeled data.\n",
    "\n",
    "* Reinforcement learning is not that different from unsupervised but there we at least have clear objective to aim for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> A computer program is said to learn from *experience* E with respect to some class of *tasks* T and *performance measure* P if its performance at tasks in T, as measured by P, improves with experience E.\n",
    "\n",
    "$T$:\n",
    "  - **Task** is how ML system should process a **collection of features**, i.e. **example** $\\boldsymbol{x} \\in \\mathbb{R}^n$\n",
    "  * classification (w/ or w/o missing values), regression, transcription, translation, anomaly detection, imputation, denoising, density estimation, ...\n",
    "  \n",
    "$P$:\n",
    "  * usually tied to the $T$\n",
    "  * accuracy (classification, ...), error (regression, ...), log-likelihood (density estimation, ...)\n",
    "\n",
    "$E$:\n",
    "  * dataset (w/ or w/o labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example: Linear Regression\n",
    "\n",
    "$$\\hat{y} = \\boldsymbol{w}^T\\boldsymbol{x}$$\n",
    "\n",
    "$T$: predict $y$ from $\\boldsymbol{x}$  \n",
    "\n",
    "$P$: $MSE_{test} = \\frac{1}{m}\\sum_i^{}(\\hat{y}_i^{(test)}-y_i^{(test)})^2$\n",
    "\n",
    "$E$: $(\\boldsymbol{X}^{(train)}, \\boldsymbol{y}^{(train)})$\n",
    "\n",
    "**Need**: ML algorithm that improves weights $\\boldsymbol{w}$ in a way that reduces $MSE_{test}$ given the training examples.\n",
    "\n",
    "**Solution**: minimize $MSE_{train}$:\n",
    "$$\\nabla_{\\boldsymbol{w}} MSE_{train} = 0$$\n",
    "$$ ... $$\n",
    "$$\\boldsymbol{w} = ({\\boldsymbol{X}^{(train)}}^T\\boldsymbol{X}^{(train)})^{-1}{\\boldsymbol{X}^{(train)}}^T\\boldsymbol{y}^{(train)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "To make this easier to understand, let's look at example of simple machine learning model - linear regression. Given training samples and labels, we want to assign labels to unseen samples. The label is predicted as linear combination of features.\n",
    "\n",
    "We fit the weights on training data and measure the performance with mean squared error on test data.\n",
    "\n",
    "It can be shown that minimizing MSE is equivalent to maximazing log-likelihood. Thus we have obtained ML estimate.\n",
    "\n",
    "<img src=\"assets/linreg.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "#### Implicit: Regularizing Matrix Inversion\n",
    "$$ \\mathbf{X}^{-1} = \\mathbf{X}^T\\mathbf{X} + \\alpha\\mathbf{I} $$\n",
    "\n",
    "---\n",
    "\n",
    "Engineering suitable feature transformation function can allow simple model to generalize well on the data even if it is not linear. Feature/Kernel Engineering is a difficult task. Manual work and expertise needed.\n",
    "\n",
    "The effect is apparent in the transformation to polar coordinates. Similrly it is apparent that feature transformations are far from trivial in any real-world example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Feature Transformations\n",
    "![Example of different representations](assets/features1.png)\n",
    "![](assets/feature_transform.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Capacity and Over-/Under- fitting\n",
    "ML != Optimization ... **generalization (test) error**\n",
    "\n",
    "model **capacity** controls whether the model is more likely to **under-** or [**over-fit**](https://www.youtube.com/watch?v=DQWI1kvmwRg)\n",
    "<!--![](assets/capacity.png)-->\n",
    "<img src=\"assets/over_under_fit.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "Caveat: Deep Learning models have theoretically unlimited capacity.\n",
    "\n",
    "Occam's Razor: Among competing hypotheses that explain known observations equally well, select the \"simplest\" one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The No Free Lunch Theorem:\n",
    "Averaged over all data-generating distributions, every classification algorithm has the same error rate when classifying previously unseen samples.\n",
    "\n",
    "BUT, in practice, we encounter only some types of data-generating distributions that are problem specific. The goal of machine learning is thus to seek machine learning algorithms that perform well on data drawn from the given distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Regularization\n",
    "\n",
    "> Any modification to a learning algorithm that is intended to reduce generalization error but not the training error.\n",
    "\n",
    "E.g. penalize weights with `L2 norm`\n",
    "$$J(\\boldsymbol{w}) = MSE_{train} + \\lambda\\boldsymbol{w}^T\\boldsymbol{w},$$ \n",
    "where $\\lambda$ is a **hyperparameter** expressing our preference over possible model functions.\n",
    "\n",
    "How to choose values of hyperparameters? \n",
    "-> train/**validation** split, e.g. 80/20\n",
    "<!-- Note that else you should have some test set that is also held out -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<!--![](assets/generalization.png)-->\n",
    "<img src=\"assets/generalization.png\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The fundamental trade-off between bias and variance - you seek a place where both are acceptably low - your mode has enough capacity to express the training data, but is not fundemntally biased towards this trainign data not to be able to predict on test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Popular ML Algorithms\n",
    "\n",
    "Supervised:\n",
    "* Linear regression, Logistic regression, LDA,  SVMs, K-Nearest neighbors, Decision trees, ...\n",
    "\n",
    "Unsupervised:\n",
    "* PCA, ICA, K-Means clustering, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[<img src=\"assets/ml_cheatsheet.png\" style=\"width: 800px;\"/>](https://cdn-images-1.medium.com/max/2000/1*dYgEs2roROf3j2ANzkDHMA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "[Rule #3: Choose machine learning over a complex heuristic.](https://developers.google.com/machine-learning/rules-of-ml/)\n",
    "\n",
    "---\n",
    "Here, go back to the landscape of AI slide. Motivate DL\n",
    "\n",
    "- assuming that data was generated by composition of factors, features, potentially at multiple levels of hierarchy.\n",
    "- Some problems do not benefit from DL. Other problems benefit immensely (object recognition, speech recognition, machine translation)\n",
    "\n",
    "Deep Learning takes some inspiration from neuroscoence, but does not try to model the way the brain works. That is a different pursuit. followed by neuroscience researchers. The domains continue to enrich each other though.\n",
    "\n",
    "Example is DanQ: a convolutional AND recurrent neural net for quantifying the function of DNA sequences. \"predicting noncoding function de novo from sequence.\"\n",
    "\n",
    "- Using deep learning to model the hierarchical structure and function of a cell\n",
    "- Removal of batch effects using distribution-matching residual networks\n",
    "- Learning a hierarchical representation of the yeast transcriptomic machinery using an autoencoder model\n",
    "- Deep biomarkers of human aging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning\n",
    "\n",
    "[]() | []()\n",
    ":---: | :---:\n",
    "<img src=\"assets/neuron_model.jpeg\" style=\"width: 400px;\"/> | <img src=\"assets/neural_net.jpeg\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "Aplications in Genomics, System Biology, Biomarker discovery, ...\n",
    "- [deeplearning-biology](https://github.com/hussius/deeplearning-biology)\n",
    "- [awesome-deepbio](https://github.com/gokceneraslan/awesome-deepbio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Architecture:\n",
    "#### Depth gives more powerful models\n",
    "\n",
    "[]() | []()\n",
    ":---: | :---:\n",
    "<img src=\"assets/depth1.png\" style=\"width: 300px;\"/> | <img src=\"assets/depth2.png\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Apart from depth and width there are other considerations in terms of architecture:\n",
    "* Connectivity of the layers, backward connections\n",
    "  * CNNs, RNNs\n",
    "* Skip connections\n",
    "  * ResNets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example: Learning XOR\n",
    "This is a toy example of **deep feedforward network** (also called **MLP**)\n",
    "\n",
    "[]() | []() | []()\n",
    ":---: | :---: | :---:\n",
    "<img src=\"assets/xor.png\" style=\"width: 200px;\"/> | <img src=\"assets/xor_net.png\" style=\"width: 200px;\"/> | <img src=\"assets/relu.png\" style=\"width: 300px;\"/>\n",
    "\n",
    "$$J(\\boldsymbol{\\theta}) = \\frac{1}{4}\\sum_{\\boldsymbol{x}}^{}(f(\\boldsymbol{x}) - \\hat{f}(\\boldsymbol{x}; \\boldsymbol{\\theta}))^2,\\quad \\hat{f}(\\boldsymbol{x};\\boldsymbol{\\theta}) = \\boldsymbol{x}^T\\boldsymbol{w} + b \\quad \\rightarrow \\quad \\boldsymbol{\\hat{y}} = \\frac{1}{2} :($$\n",
    "\n",
    "Takeaway: Linear model can learn non-linear function via feature transformations. Instead of engineering it, you can learn it. Usually, by specifying some broader family of functions and tuning on the data.\n",
    "\n",
    "\n",
    "$$\\boldsymbol{h}=g(\\boldsymbol{W}^T\\boldsymbol{x}+\\boldsymbol{c}),$$\n",
    "where $h$ is output of hidden unit.\n",
    "$$\\hat{f}(\\boldsymbol{x}; \\boldsymbol{\\theta})= \\boldsymbol{w}^T \\mathrm{max}\\{0,\\boldsymbol{W}^T\\boldsymbol{x}+\\boldsymbol{c}\\}+b$$\n",
    "\n",
    "Here we have used **Rectified Linear Unit (ReLU)** as nonlinearity $g(\\cdot)$ on the hidden layer.\n",
    "\n",
    "---\n",
    "Solution:\n",
    "$$\n",
    "\\boldsymbol{W} = \\begin{bmatrix}\n",
    "1 & 1\\\\\n",
    "1 & 1\\\\\n",
    "\\end{bmatrix},\n",
    "\\qquad\n",
    "\\boldsymbol{c} = \\begin{bmatrix}\n",
    "0\\\\\n",
    "-1\\\\\n",
    "\\end{bmatrix},\n",
    "\\qquad\n",
    "\\boldsymbol{w} = \\begin{bmatrix}\n",
    "1\\\\\n",
    "-2\\\\\n",
    "\\end{bmatrix},\n",
    "\\qquad b = 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note: There is plenty of activation functions, but ReLU is prefered.\n",
    "<img src=\"assets/activations.png\" style=\"width: 300px;\"/>\n",
    "\n",
    "> Most deep nets nowadays use ReLU for hidden layers because it avoids the vanishing gradient problem and it is faster to train than alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Else there are Leaky ReLUs or Paramatric ReLUs that have nonzero gradient for x<0.\n",
    "\n",
    "ReLUs are inspired by biological neurons and how they respond to stimuli.\n",
    "ReLUs are not differentiable at 0, so we take right-sided derivative instead (set the value to 0). This is how the software implementation (Tensorflow, Theano, ...) handles the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Backpropagation\n",
    "> Backpropagation is an algorithm that computes the chain rule of derivatives, with a specific order of computations that is highly efficient.\n",
    "\n",
    "> The derivative on each variable tells you the sensitivity of the whole expression on its value.\n",
    "\n",
    "## Chain rule\n",
    "$$y=g(x) \\qquad z=f(g(x))=f(y)\\rightarrow \\frac{dz}{dx}=\\frac{dz}{dy}\\frac{dy}{dz}$$\n",
    "\n",
    "## Backprop as computational graph:\n",
    "<img src=\"assets/backprop_graph.png\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* nodes = variables, edges = ops.\n",
    "* DL Libraries like Thean and Tensorflow are built with computaitonal graphs in mind. They build the graph of backprop and they can well optimize its execution.\n",
    "\n",
    "* The backprop algorithm needn't to access numeric values. Instead, it adds nodes to computational graph symbolically decribing how to obtain these derivatives.\n",
    "  * a graph evaluation engine then evaluates every node as soon as its parent become available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Forward Propagation for MLP as Graph\n",
    "\n",
    "Objective:\n",
    "\n",
    "$$J = J_{MLE} + \\lambda\\bigg(\\sum_{i,j}^{}\\big(W_{i,j}^{(1)}\\big)^2 + \\sum_{i,j}^{}\\big(W_{i,j}^{(2)}\\big)^2\\bigg)$$\n",
    "\n",
    "<img src=\"assets/mlp_graph.png\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Backprop in Fully Connected Feed Forward Net\n",
    "### Forward propagation:\n",
    "<img src=\"assets/forprop.png\" style=\"width: 400px;\"/>\n",
    "### Backprop:\n",
    "<img src=\"assets/backprop.png\" style=\"width: 450px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Feedforward nets do not have feedback connections (as RNNs do). \n",
    "\n",
    "Backpropagation is **merely** algorithm to compute gradient. Learning is done separately (see *SGD*, RMSProp, Adam, Adagrad,...). Note that this is just the old same gradient based learning.\n",
    "\n",
    "Note that the pseudocode is on single sample. Normally we run it on sampled minibatches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regularization for DL\n",
    "\n",
    "> Deep learning algorithms are typically applied to extremely\n",
    "complicated domains such as images, audio sequences and text, for which the true generation process essentially involves simulating the entire universe.  \n",
    "What this means is that controlling the complexity of the model is not a\n",
    "simple matter of ﬁnding the model of the right size, with the right number of parameters. Instead, we might ﬁnd - and indeed in practical deep learning scenarios, we almost always do ﬁnd - that **the best fitting model (in the sense of minimizing generalization error) is a large model that has been regularized appropriately**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Parameter Norm Penalties\n",
    "$$\\tilde{J}(\\boldsymbol{\\theta}; \\mathbf{X},\\mathbf{y}) = J(\\boldsymbol{\\theta};\\mathbf{X},\\mathbf{y}) + \\lambda\\Omega(\\boldsymbol{\\theta}),$$\n",
    "express prior belief that the weights should be small and/or sparse. [Constraints](https://keras.io/constraints/) and [Regularizers](https://keras.io/regularizers/)\n",
    "\n",
    "\n",
    "``` python\n",
    "from keras.regularizers import l1_l2\n",
    "# Adds regularization term to cost function\n",
    "model.add(Dense(64, input_dim=64, kernel_regularizer=l1_l2(0.2)\n",
    "```\n",
    "\n",
    "``` python\n",
    "from keras.constraints import max_norm # l2 norm\n",
    "# Directly applies scaling to weights\n",
    "model.add(Dense(64, kernel_constraint=max_norm(2.)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Sparse Representations\n",
    "Express prior belief on sparse activation outputs\n",
    "``` python\n",
    "from keras.regularizers import l1_l2\n",
    "model.add(Dense(64, input_dim=64, activity_regularizer=l1(0.2)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "$$L_2: \\Omega(\\boldsymbol{\\theta}) = \\frac{1}{2}||\\boldsymbol{w}||_2^2 = \\frac{1}{2}\\sum_{i}^{}(w_i)^2, \\qquad $$\n",
    "$$L_1: \\Omega(\\boldsymbol{\\theta}) = \\frac{1}{2}\\sum_{i}^{}|w_i|. $$\n",
    "\n",
    "<img src=\"assets/lasso.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "* usually penalize only weigths, not biases `bias_constraint`\n",
    "* may penalize layers with different $\\alpha$\n",
    "* combining `l1` and `l2` is [elastic net](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html)\n",
    "* `l1` is lasso and does feature selection\n",
    "\n",
    "\n",
    "* Sparse representations: Puts a penalty on activation outputs. This leads to sparse representations of features and indirectl poses complicated penalty on weights.\n",
    "  * Biases are usually not reguralized, but can be done in same fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dataset Augmentation\n",
    "Includes noise injection to inputs, hidden layer weights, targets. [Image Preprocessing](https://keras.io/preprocessing/image/)\n",
    "``` python\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "model = deep_nn() # defined elswhere\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# compute quantities required for featurewise normalization\n",
    "datagen.fit(x_train)\n",
    "\n",
    "# fits the model on batches with real-time data augmentation:\n",
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size=32),\n",
    "                    steps_per_epoch=len(x_train) / 32, epochs=epochs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* Adversarial Training is form of dataste augmentation when we try to construct such examples that make the net fail.\n",
    "* It acts as an regularizer in that it encourages the net to be locally constant in the neigborhood if the training data. Thus avderisal training is a way how to introduce constancy prior.\n",
    "\n",
    "<img src=\"assets/adversarial-example.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Early Stopping\n",
    "Return parameters that gave the lowest validation set loss. [Early Stopping](https://keras.io/callbacks/#earlystopping)\n",
    "``` python\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import SGD\n",
    "#define model\n",
    "model = deep_nn() # this is defined elsewhere\n",
    "model.compile(optimizer=SGD, loss='mse', metrics=['accuracy'])\n",
    "# define early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_acc', min_delta=0.0001,\n",
    "                          patience=5, verbose=1, mode='auto')\n",
    "callbacks_list = [earlystop]\n",
    "# train the model\n",
    "model_info = model.fit(train_features, train_labels,batch_size=128,\n",
    "                       nb_epoch=100, callbacks=callbacks_list,\n",
    "                       verbose=0,validation_split=0.2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* Early stopping is simple to implement, you get it anyway\n",
    "* Number of steps for training is a hyperparameter as well \n",
    "* Things become more difficult if your training is unsupervised\n",
    "\n",
    "<img src=\"assets/early_stopping.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "* Early stopping is the most popular form of regularization in DL\n",
    "  * treats number of steps as hyperparameter (n* is optimal)\n",
    "  * cheap to optimize, cost mainly only from getting loss on validation set\n",
    "  * should refit on the whole train+valid set after picked weights from early stopping\n",
    "    * either reinitialize, pool data and train for n* steps\n",
    "    * or train from where you are and wait till error drops to the lowest one observed previously (which may not happen)\n",
    "  * Regularization done by limiting the param. space the NN can explore. In this sense it acts similarly as parameter norms and constraints, but the amount of regularization is obtained from the traning phase, without need for some apriori or search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Parameter Sharing\n",
    "Forces parameter sets to be equal.\n",
    "```python\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D,GlobalAveragePooling1D,MaxPooling1D\n",
    "\n",
    "seq_length = 64\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(64, 3, activation='relu', input_shape=(seq_length, 100)))\n",
    "model.add(Conv1D(64, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=16, epochs=10)\n",
    "score = model.evaluate(x_test, y_test, batch_size=16)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* Parameter sharing allows us not to store all the params, only the unqiue ones.\n",
    "  * Expresses prior that only bunch of pixels have meaning, not single pixels.\n",
    "  * Helps to achieve translational invariance\n",
    "  * Used in Convolutional Neural nets, but that is not the only domain. Sequences, texts as well!\n",
    "    * basically expresses the prior that features learned in one place are useful also elswhere (e.g. edges in images)\n",
    "  \n",
    "Together with local connectivity this is one of the two main features of CNNs.\n",
    "  * more on that some time later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model Averaging\n",
    "> Any machine learning algorithm can benefit substantially from model averaging (e.g. bagging) at the price of increased computation and memory. Machine learning competitions are usually won by methods using model averaging over dozens of models.\n",
    "\n",
    "``` python\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100,learning_rate=1.0,\n",
    "                                 max_depth=1, random_state=0).\\\n",
    "                                fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)                 \n",
    "> 0.913\n",
    "```\n",
    "\n",
    "Check also [Keras Lambda](https://keras.io/layers/core/#lambda) layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"assets/ensembl.png\" style=\"width: 500px;\"/>\n",
    "<img src=\"assets/ensembl2.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* The goal of ensemble methods is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator.\n",
    "  * Bagging ... average of estimators built independently (they should be as uncorrelated as possible). It acts as regularizer in that it reduces variance of etimates\n",
    "  * Boosting ... weighted average of estimators built sequentially (sort of mixture of experts) ... leads to increased model capacity\n",
    "  * JMD may tell you more about this if he talks about Dueling Networks\n",
    "  * NNs can be bagged even if trained on same datastet with same objective. There is already so much happening under the hood (random initialization, minibatches sampling, hyperparams seelction,...) that the errors made by such nets will bepartially independentn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dropout\n",
    "Very effective and simple regularization technique. To a first approximation, dropout is a method for making bagging practical for very many and large NNs.\n",
    "``` python\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dropout, Activation\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(filters = 32, kernel_size = (8, 8), \n",
    "                        strides = (4, 4), input_shape = img_size + (num_frames, )))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "\"\"\" \n",
    "Vanilla Dropout\n",
    "We drop and scale at train time and don't do anything at test time.\n",
    "\"\"\"\n",
    "p = 0.5 # prob of keeping a unit active. higher = less dropout\n",
    "\n",
    "def train_step(X):\n",
    "  # forward pass for example 3-layer neural network\n",
    "  H1 = np.maximum(0, np.dot(W1, X) + b1)\n",
    "  U1 = (np.random.rand(*H1.shape) < p) / p # first dropout mask.\n",
    "  H1 *= U1 # drop\n",
    "  H2 = np.maximum(0, np.dot(W2, H1) + b2)\n",
    "  U2 = (np.random.rand(*H2.shape) < p) / p # second dropout mask.\n",
    "  H2 *= U2 # drop\n",
    "  out = np.dot(W3, H2) + b3\n",
    "  \n",
    "  # backward pass: compute gradients... (not shown)\n",
    "  # perform parameter update... (not shown)\n",
    "def predict(X):\n",
    "  # ensembled forward pass\n",
    "  H1 = np.maximum(0, np.dot(W1, X) + b1)\n",
    "  H2 = np.maximum(0, np.dot(W2, H1) + b2)\n",
    "  out = np.dot(W3, H2) + b3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Batch Normalization\n",
    "\n",
    "Applies a transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1.\n",
    "See [docs](https://keras.io/layers/normalization/), and be [careful about batch sizes](https://www.alexirpan.com/2017/04/26/perils-batch-norm.html)\n",
    "``` python\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dropout, Activation\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=20))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Regularization Checkpoint:\n",
    "\n",
    "* Use `Dropout`, `BatchNormalization`, `EarlyStopping` and `l2`\n",
    "* Center and scale inputs, augment if possible\n",
    "* Select appropriate loss function\n",
    "  * classification: `categorical_crossentropy`, `squared_hinge`\n",
    "  * regression: [`huber_loss`](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b), `MSE` (l2 loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* Optimizing L2 loss is more dificiult than the classification losses. Whenever possible, think if you can substitute regression by classification.\n",
    "* The squared loss function results in an arithmetic mean-unbiased estimator, and the absolute-value loss function results in a median-unbiased estimator (in the one-dimensional case, and a geometric median-unbiased estimator for the multi-dimensional case). The squared loss has the disadvantage that it has the tendency to be dominated by outliers. Huber loss combines best of the two rolds\n",
    "\n",
    "Batch norm changes models in two fundamental ways.\n",
    "* At training time, the output for a single input xi depends on the other xj in the minibatch.\n",
    " * At testing time, the model runs a different computation path, because now it normalizes with the moving average instead of the minibatch average.\n",
    "\n",
    "See paper on [batch renormalization](https://arxiv.org/abs/1702.03275)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* \"The best way how to make a machine learning model generalize better is to train it on more data. Of course, this is easier said than done on partice. One way how to get around the problem is using fake data.\" For some tasks, it is more straightforward to create fake data then for others. Classification is such task.\n",
    "  * This includes also injecting additive noise to the input\n",
    "  * Dataset augumentation techniques are hand-designed :)\n",
    "  \n",
    "* Dropout - as simple to implement as it gets\n",
    "  * used only during training.\n",
    "  * 0.5 is good place to start\n",
    "  * sort of like bagging, but models are not independen (they share params, which ismemory-benefitial)\n",
    "  * it is more effective than weight decay, activity reguralizers, ...\n",
    "  * can be expensive (despite being cheap to apply). Reduces model capacity -> need larger model and more training -.-\n",
    "  * Drop out is less effective when few traning examples available.\n",
    "  * Droput reguralizers hiden units to be good features in VARIETY of contexts\n",
    "  * Droupout is multiplicative noise on hidden units. It works as adaptive destruction of the information content in the input\n",
    "  * May want to draw a picture!\n",
    "* BatchNormalization: mainly to aid optimization but also introduces both additive and multiplicative noise on hidden units\n",
    "  *  batch normalization can be interpreted as doing preprocessing at every layer of the network, but integrated into the network itself in a differentiable manner. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimization for DL Training\n",
    "\n",
    "> Of all the many optimization problems involved in DL, the most difficult is NN training. It is quite common to invest days to months of time on hundreds of machines to solve even a single instance of the NN training problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Selecting Minibatch Size\n",
    "* larger batches estimate gradient more accurately, but with less then linear returns\n",
    "* batch should not be too small to better use hardware resources, but not too big to be able to fit to memory\n",
    "* GPUs tend to prefer power 2 sized batches\n",
    "\n",
    "#### Notes:\n",
    "* Batches are sampled randomly\n",
    "* Should shuffle the set, if data has some temporal correlation.\n",
    "* Run several `epochs`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The goal: finding parameters $\\theta$ that siginficantly reduce a cost function $J(\\boldsymbol{\\theta})$, which typically includes a performance measure evaluated on the entire training set as well as additional regularization terms.\n",
    "* Optimization algorithms used for training of deep models diﬀer from traditional optimization algorithms in several ways\n",
    "  * ML acts indirectly we reduce J in hope of also optimizing some P that is often intractable. In optimization, we just go for optimizing J.\n",
    "  * additional problems arrise if your objective is non differentiable (e.g. class assignement as piecewise constant) -> **surrogate objective function**\n",
    "  * We also practice early stopping, not going for minumun of J\n",
    "  * Loss ususally decomposes over samples and you compute it batch-wise\n",
    "  \n",
    "* Batches randomly, But Prioritized Experience Replay may be advantageous over random batches sampling.\n",
    "\n",
    "* Epohes are passes over the set. Only the first pass is theoretically guaranteed to improve the generalization error so should not run too many times, unless overfitting. (E.g. Montior you validation loss.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Minima, Saddles and Cliffs\n",
    "\n",
    "> Nearly any deep model is essentially guaranteed to have an extremely large number of local minima.\n",
    "\n",
    "> For many high-dimensional nonconvex functions, local minima (and maxima)\n",
    "are in fact rare compared to another kind of point with zero gradient: a saddle point.\n",
    "\n",
    "- Plot norm of the gradient over time!\n",
    "\n",
    "#### Gradient Clipping\n",
    "<img src=\"assets/clipping.png\" style=\"width: 400px;\"/>\n",
    "`clipnorm` and `clipvalue` can be used with all optimizers\n",
    "\n",
    "``` python\n",
    "from keras import optimizers\n",
    "# All parameter gradients will be clipped to:\n",
    "# a maximum value of 0.5 and and a minimum value of -0.5.\n",
    "sgd1 = optimizers.SGD(lr=0.01, clipvalue=0.5)\n",
    "# a maximum norm of 1.\n",
    "sgd2 = optimizers.SGD(lr=0.01, clipnorm=1.)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* Less of a problem as many minima arise from model nonidentifiability (shufling inputs, scaling parameters) and are equivalent\n",
    "\n",
    "* Plateaus, Saddles, ...\n",
    "\n",
    "* Cliffs can countreact learning.\n",
    "  * They are frequent with RNNs as there we apply matrix multiplication by the same matrix repeatedly.\n",
    "  \n",
    "* Similarly, gradients could often vanish in RNNs, to address this, LSTMs (Long-short-temr-memory unit) and GRUs (gated recurrent unit)\n",
    "  * In FF nets, use skip connections (as in res nets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Parameter Initialization\n",
    "\n",
    "> Usually, we set the biases for each unit to heuristically chosen constants and initialize only the weights randomly.\n",
    "\n",
    "> If computational resources allow it, it is usually a good idea to treat the scale of the weights for each layer as a hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* Greedy supervised pretraining is often used. \n",
    "\n",
    "* may use unsupervised learning and same data to get initial model parameters for supervised learning.\n",
    "    * Even prtraining on unrelated task may be helpful. Makes units look for different features, sets correct scale, ...\n",
    "    * Greedy layer-wise unsupervised pretraining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Optimizer Algorithms for ML\n",
    "\n",
    "SGD, RMsprop, Adagrad, Adadelta, Adam, Adamax, Nadam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "* Classical Gradient Descent: $$w_j := w_j - \\alpha \\frac{\\partial J(\\boldsymbol{w})}{\\partial w_j},$$ where $\\alpha$ is *learning rate*, is of $\\mathcal{O}(m)$\n",
    "* Loss decomposes as sum over samples\n",
    "$$J(\\boldsymbol{w}) = \\frac{1}{m}\\sum_i^{m}(\\hat{y}_i-y_i)^2 \\xrightarrow[]{\\nabla_{\\boldsymbol{w}}} \\nabla_{\\boldsymbol{w}} J(\\boldsymbol{w}) = \\frac{2}{m} \\boldsymbol{X}^T (\\boldsymbol{\\hat{y}}-\\boldsymbol{y})$$\n",
    "* Insight: gradient is expectation that can be estimated on subset of samples\n",
    "  * Draw (uniformly) a fixed-sized **minibatch**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* Powers nearly all of deep learning.\n",
    "* SGD useful when cost function can be decomposed as sum over examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"assets/pseudo_sgd.png\" style=\"width: 800px;\"/>\n",
    "\n",
    "#### Adaptive Learning Rate\n",
    "In practice, anneal learning rate linearly until iteration $\\tau$, then keep constant:\n",
    "$$\\epsilon_k = (1-\\alpha)\\epsilon_0 + \\alpha\\epsilon_\\tau$$\n",
    "```python\n",
    "from keras.optimizers import SGD\n",
    "sgd = SGD(decay = 1e-6)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Momentum and Nesterov Momentum\n",
    "$\\theta \\leftarrow \\theta - \\epsilon_k\\hat{\\boldsymbol{g}}$ becomes:\n",
    "$$\\boldsymbol{v} \\leftarrow \\alpha \\boldsymbol{v} - \\epsilon_k\\hat{\\boldsymbol{g}}, \\qquad \\theta \\leftarrow \\theta + \\boldsymbol{v}$$\n",
    "\n",
    "Update step is larger if experienced gradients point consistently in one direction. Counteracts getting stopped in regions of low gradient.\n",
    "\n",
    "<img src=\"assets/nesterov.jpeg\" style=\"width: 700px;\"/>\n",
    "\n",
    "``` python\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Other Optimizers\n",
    "RMsprop, Adagrad, Adadelta, Adam, Adamax, Nadam\n",
    "\n",
    "[]() | []()\n",
    ":---: | :---:\n",
    "<img src=\"assets/optimizers.gif\" style=\"width: 300px;\"/> | <img src=\"assets/optimizers1.gif\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "May want to use adaptive learning rate. Minibatch usually MUCH smaller than the size of dataset ... quicknes the computation  (or even makes it possible).\n",
    "\n",
    "Stopping criterion? - usually by monitoring loss on validation set.\n",
    "\n",
    "Adaptive LR available also with other optimziers. But here more advanced approaches taken to speed up training, avoid getting stuck,...\n",
    "  * Much like Adam is essentially RMSprop with momentum, Nadam is Adam RMSprop with Nesterov momentum.\n",
    " \n",
    "* RMSprop used in the DeepMindNature paper http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Supervised Pretraining\n",
    "\n",
    "* Train each layer separately\n",
    "* Train each layer using as output of previously trained layer as input\n",
    "* Train deep model, keep only $n$, $m$ layers on input and output, fill in between with randomly initialized layers to make even deeper model\n",
    "\n",
    "#### Transfer Learning\n",
    "\n",
    "* Train model on some task, keep $k$ first layers, retrain on different task (possibly with fewer samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Skip Connections\n",
    "<img src=\"assets/skipconnects.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "* [ResNets](https://www.youtube.com/watch?v=K0uoBKBQ1gA)\n",
    "\n",
    "## (Stochastic) Curriculum Learning\n",
    "* Give the net random mix of easy and difficult examples, increase proportion of difficult ones over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* Supervised pretraining is akin to Transfer Learning\n",
    "\n",
    "* Skip Connections reduce the length of the shortest path from layers to output. This facilitates backpropagation and avoids vanishing gradient problem (gradient too small and thus uninformative). This problem arises with squashing funcs (fine as we avoid them) and in deep nets (due to repeated ops through layers driivng values <1 lower and lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Optimization for DL summary:\n",
    "\n",
    "* Initialize layer weights from normal distribution, preferably `he_normal`\n",
    "  * or check values of gradients on single minibatch, adjust scale of initial weights accordingly\n",
    "  * or initialize (some) weights with supervised pretraining\n",
    "* use `Adam` or `SGD` w/ *momentum*\n",
    "* use *gradient clipping*\n",
    "* use *adaptive learning rate*\n",
    "* select model type according to established practice (CNNs, RNNs, ResNets, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Practical Methodology (cont'd)\n",
    "\n",
    "> In practice, one can usually do much better with a correct application of a commonplace algorithm than by sloppily applying an obscure algorithm.\n",
    "\n",
    "* [Advice for applying Machine Learning](https://see.stanford.edu/materials/aimlcs229/ML-advice.pdf)\n",
    "* [Rules of Machine Lerning](https://developers.google.com/machine-learning/rules-of-ml/)\n",
    "\n",
    "### Recipe\n",
    "* Determine your goals (performance metric and their target value)\n",
    "* Establish baseline implementation of the end-to-end pipeline ASAP\n",
    "* Use logging, callbacks and [visualizations](https://www.tensorflow.org/versions/r1.0/get_started/summaries_and_tensorboard) generously to determine bottlenecks\n",
    "* Iterate with incremental changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Performance Metrics\n",
    "``` python\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='sgd', metrics=['mae', 'acc'])\n",
    "```\n",
    "* use multiple, often problem specific\n",
    "  * Can report F1 score: $$F = \\frac{2pr}{p+r}$$\n",
    "  * also Coverage, AUC\n",
    "* `loss` != `metrics`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* Alternative is reporting area under precision recall curve.\n",
    "* Draw precision recall curve, TPR / FPR\n",
    "<img src=\"assets/roc.png\" style=\"width: 400px;\"/>\n",
    "* Coverage: How many samples can you make a decision on autonomously (ahving human assistance is not that bad thing, also if you can feed in the information)\n",
    "  * E.g. Google Street View House Number Transcription had 95% coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Callbacks\n",
    "``` python\n",
    "import numpy as np\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    \n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "class Metrics(Callback):\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    " \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = np.asarray(self.model.predict(\\\n",
    "                            self.model.validation_data[0])).round()\n",
    "        y_true = self.model.validation_data[1]\n",
    "        _val_f1 = f1(y_true, y_pred)\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        return\n",
    " \n",
    "metrics = Metrics()\n",
    "...\n",
    "model.fit(training_data, training_target, \n",
    "          validation_data=(validation_data, validation_target),\n",
    "          nb_epoch=10, batch_size=64, callbacks=[metrics])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Baseline Prototype\n",
    "\n",
    "* Pick appropriate model (recall *Occam's Razor*, don't reinvent the wheel)\n",
    "\n",
    "Model | Feedforward | CNN | RNN\n",
    ":--- | ---: | ---: | ---:\n",
    "Input | fixed sized vector | topological structure | sequence\n",
    "\n",
    "*  As a sanity check, make sure your initial loss is reasonable, and that you can achieve 100% training accuracy on a very small portion of the data\n",
    "* Use available [datasets](http://deeplearning.net/datasets/) and [models](https://github.com/tensorflow/models) to your advantage\n",
    "* Use model ensembles for extra performance\n",
    "* During training, monitor the loss, the training/validation accuracy, the magnitude of updates in relation to parameter values (it should be ~1e-3), and when dealing with ConvNets, the first-layer weights.\n",
    "* Use unsupervised pre-training (domain dependent)\n",
    "\n",
    "Additionally, use all that mentioned with **Optimizers** and **Regularization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* Pick simplest model possible, don't do DL if not necessary\n",
    "  * It should be clear to you that successful application of DL requires quite some work.\n",
    "  * Rule #1: Don’t be afraid to launch a product without machine learning.\n",
    "  * Rule #3: Choose machine learning over a complex heuristic.\n",
    "* Unsupervised pretraining should be used if your domain uses it.\n",
    "* If your error on training set is higher than target error rate, you have no choice but to increase model capacity. If not regularized, must add layers, units.\n",
    "* neural nets typically perform best if the training error is very low. The   generalization gap then should be closed by proper regularization.\n",
    "* Setting up all the callbacks, plotting, reporting etc is time consuming, but it is largely one-time effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Do I need more data?\n",
    "\n",
    "> Many ML novices are tempted to make improvements by trying out many different algorithms. Yet, it is often much better to gather more data than to improve the learning algorithm.\n",
    "\n",
    "* performance on training set poor => more data won't help\n",
    "* test set performance poor & train set performance good => get more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### How much data do I need?\n",
    "\n",
    "<img src=\"assets/accuracy_samples.png\" style=\"width: 450px;\"/>\n",
    "> Usually, adding a small fraction of the total number of examples will not have noticeable on generalization error. As a rule of thumb, aim at least at doubling the training set size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Clearly, you should always get as much as possible. Of course, this is limited.\n",
    "* Poor performance on trainign set\n",
    "  * bigger model\n",
    "  * tune hyperparams\n",
    "  * if doesnt work - data likely bad quality - get new data.\n",
    "* Poor performance on test set\n",
    "  * if getting more data too expensive (e.g. medical domain) -> reduce model capacity = smaller model, regularization\n",
    "    * If doesnt work, should get more data\n",
    "  * The amount of data -- get it by extrapolating the learning curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Selecting hyperparameters\n",
    "Manual vs. Automatic\n",
    "\n",
    "> If you have time to tune only one hyperparameter, tune the learning rate.\n",
    "\n",
    "> NN can sometimes perform well with only a small number of tuned hyperparameters, but often benefit significantly from tuning forty or more.\n",
    "\n",
    "Random Search > Grid Search\n",
    "<img src=\"assets/hyperparam_search.jpeg\" style=\"width: 400px;\"/>\n",
    "\n",
    "Example:\n",
    "$$\\texttt{log_learning_rate} \\tilde{} U(-1, -5),$$\n",
    "$$\\texttt{learning_rate} = 10^{\\texttt{log_learning_rate}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* Can do it manually, but need experience and deeper understanding\n",
    "* The effective capacity of the model is highest when the `lr` is *correct*\n",
    "* Size and number of hidden layers is also a hyperparameter\n",
    "\n",
    "* Examples of hyperparameters:\n",
    "  * Learbing rate, loss function, minibatch size, number if training iterations/epohcs, momentum \n",
    "  * Preprocessing parameters - channels, image size, scaling, centering, ...\n",
    "  * number of hidden units, convoltuion kernel size, weight decay, dropout,  nonlinearity, activation sparsity, weight initialization, model averaging, Batch Normalization, Pooling\n",
    "\n",
    "* Prefer Random Search over grid Search - avoids wasting computation on unimportant parameters.\n",
    "  * Define distribution over parameters.\n",
    "  * Should be on log-scale\n",
    "  * Should run it iteratively to achieve better granularity\n",
    "  \n",
    "* Can also try Model-Based methods\n",
    "  * run set of experiments and adjust upcoming ones baed on results of previous ones [paper](https://arxiv.org/abs/1406.3896)\n",
    "  * Optimization problem\n",
    "  * field not estabilished yet\n",
    "  * Bayessian methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Debugging Strategies for ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Machine learning systems are difficult to debug [...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"assets/still-waiting-for-my-neural-network-to-train.jpg\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Visualize model in action\n",
    "* Visualize the worst mistakes\n",
    "* Check training and test errors\n",
    "  * Bias / Variance trade-off\n",
    "* Fit a tiny dataset\n",
    "* Monitor histograms of activations and gradients\n",
    "* Prototype, fail & improve\n",
    "\n",
    "High Variance (overfit) | High Bias (bad/few features)\n",
    ":---: | :---:\n",
    "<img src=\"assets/highvariance.png\" style=\"width: 300px;\"/> | <img src=\"assets/highbias.png\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- Often you don't know appriori what the inteded behaviour of the model is. You are using ML/DL to help you undestanding sth what cannot specify yourself.\n",
    "\n",
    "- DL/ML models are assemblage of parts that are adaptive. If A is wrong B,C,D often make up for it.\n",
    "  * Suppose you implement gradient update rule on biases and do it erroneously such that thet become negative $b \\leftarrow b-\\alpha$. You may not figur out as weights may adapt\n",
    "  1. Fit on data that is so simple (small) that you can predict correct behavior\n",
    "  2. Test just part of the net in isolation.\n",
    "  \n",
    "\n",
    "Visualize model in action:\n",
    "  - look at your agent playing a game\n",
    "  - listen to speech samples of a generative model\n",
    "\n",
    "Visualize the worst mistakes\n",
    "  - get confidence measure for a training example and look at those that are the worst cases of misslasification (very confident about a different label) or that the model has troubles deciding about (not really confident about any label). You should be picking up the latter anyway.\n",
    "  \n",
    "Check training and test errors:\n",
    "  - If cannot get low training error: something must be wrong.\n",
    "  - If train err low but test high:\n",
    "    * Overfitting, need to regularize\n",
    "    * Alternativelly, check that you set up your test model correctly (Droputs, BN, weight loading, hyperparams, ...)\n",
    "\n",
    "\n",
    "\n",
    "  - check question: what I need to do to adress the overfitting issue:\n",
    "    - regularize, fewer features, more trainign examples\n",
    "    \n",
    "Fit a tiny dataset:\n",
    "* you must be able to correctly classify single example / correctly reproduce a single example (autoencoder) / emit samples similar to one example (generative nets)\n",
    "\n",
    "Monitor histograms of activations and gradients\n",
    "- possibly over one epoch\n",
    "- can also monitor relative value of parameter update to parameter (should be like ~.01 - .001 )\n",
    "- note that for sparse data (like in NLP) some parts of net updated only very infrequently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Frameworks for DL\n",
    "<img src=\"assets/frameworks.png\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "(out all of this, what you should remember)\n",
    "\n",
    "General:\n",
    "* Good features need good resources\n",
    "* Callbacks, logging and [visualizations](https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard) are must from very start\n",
    "* Use existing data and models to your advantage\n",
    "* Building models with [`keras`](https://keras.io/) is easy\n",
    "\n",
    "Regularization:\n",
    "* Use `Dropout`, `BatchNormalization`, `EarlyStopping` and `l2`\n",
    "* Center and scale inputs, augment if possible\n",
    "* Select appropriate loss function (`categorical_crossentropy`, [`huber_loss`](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b), ...)\n",
    "\n",
    "Optimization:\n",
    "* use `Adam` or `SGD` w/ *momentum*; *gradient clipping*; *adaptive lr*\n",
    "* initialize weights properly\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Other highly relevant topics\n",
    "* CNNs\n",
    "* RNNs\n",
    "* Reinforcement Learning (JMD)\n",
    "* Autoencoders\n",
    "* DL as applied to genomics\n",
    "* Transfer Learning\n",
    "* Computational Graphs\n",
    "\n",
    "## Other interesting topics\n",
    "* Model Compression\n",
    "* Neuromorphic Engineering\n",
    "* Quantum Computing for ML/DL\n",
    "* Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References\n",
    "- http://www.deeplearningbook.org/\n",
    "- http://cs229.stanford.edu/materials/ML-advice.pdf\n",
    "- http://cs231n.github.io\n",
    "\n",
    "# Reading\n",
    "- http://blog.dennybritz.com/2017/01/17/engineering-is-the-bottleneck-in-deep-learning-research/\n",
    "- https://developers.google.com/machine-learning/rules-of-ml/\n",
    "- https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b463\n",
    "- http://web.mit.edu/16.070/www/lecture/big_o.pdf\n",
    "- https://www.tensorflow.org/versions/r1.0/get_started/summaries_and_tensorboard\n",
    "- https://anvaka.github.io/rules-of-ml/\n",
    "- https://databricks.com/session/deep-learning-with-apache-spark-and-gpus\n",
    "- [deeplearning-biology](https://github.com/hussius/deeplearning-biology)\n",
    "- [awesome-deepbio](https://github.com/gokceneraslan/awesome-deepbio)\n",
    "- https://github.com/tensorflow/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Network - MWE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# One hot encoding\n",
    "def one_hot_encode_object_array(arr):\n",
    "    '''One hot encode a numpy array of objects (e.g. strings)'''\n",
    "    uniques, ids = np.unique(arr, return_inverse=True)\n",
    "    return np_utils.to_categorical(ids, len(uniques))\n",
    "\n",
    "train_y_ohe = one_hot_encode_object_array(train_y)\n",
    "test_y_ohe = one_hot_encode_object_array(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.83\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "iris = load_iris()\n",
    "X = iris[\"data\"]\n",
    "y = iris[\"target\"]\n",
    "train_X, test_X, train_y, test_y = \\\n",
    "    train_test_split(X, y, train_size=0.6, test_size=0.4,\n",
    "                     shuffle=True, random_state=0)\n",
    "lr = LogisticRegressionCV()\n",
    "lr.fit(train_X, train_y)\n",
    "print(\"Accuracy = {:.2f}\".format(lr.score(test_X, test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.97\n"
     ]
    }
   ],
   "source": [
    "# Toy Feedforward net\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(16, input_shape=(4,)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "model.fit(train_X, train_y_ohe, epochs=100, batch_size=1, verbose=0)\n",
    "\n",
    "loss, accuracy = model.evaluate(test_X, test_y_ohe, verbose=0)\n",
    "print(\"Accuracy = {:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Important!** This is super simplistic, normally you should include the recommended regularizers, optimization settings, callbacks, visualizations, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We have likely overfitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 1.8.0\n",
      "Keras version: 2.2.0\n",
      "Keras config: {'floatx': 'float32', 'epsilon': 1e-07, 'backend': 'tensorflow', 'image_data_format': 'channels_last'}\n"
     ]
    }
   ],
   "source": [
    "# Report Settings\n",
    "from keras import __version__ as K_ver\n",
    "from keras.backend import _config as K_cf\n",
    "from tensorflow import __version__ as tf_ver\n",
    "print(\"Tensorflow version: {}\".format(tf_ver))\n",
    "print(\"Keras version: {}\".format(K_ver))\n",
    "print(\"Keras config: {}\".format(K_cf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tensorboard Example\n",
    "Accuracy | Cost\n",
    ":---: | :---:\n",
    "<img src=\"assets/accuracy.png\" style=\"width: 300px;\"/> | <img src=\"assets/cost.png\" style=\"width: 300px;\"/>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
