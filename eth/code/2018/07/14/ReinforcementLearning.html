<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width initial-scale=1" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>Reinforcement Learning, Teaching AI to Play</title>
    <meta name="description" content="Personal Blog">

    <link rel="stylesheet" href="/css/main.css">
    <link rel="canonical" href="http://localhost:4000/eth/code/2018/07/14/ReinforcementLearning.html">

    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
	<link rel="stylesheet" href="/css/academicons.css"/>
    <link href="/css/lightbox.css" rel="stylesheet">

    <link rel="stylesheet" href="/css/zenburn.css"/>

	<link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Martin Holub Blog" />
</head>


  <script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
	</script>

  <body>

    <header class="site-header">

  <div class="wrapper">

    <nav class="site-nav">

      <div class="trigger">
        <!--  instead of blog -->
        <a class="page-link" href="/">about</a>

        <!-- ( for page in site.pages )  was previously-->
        
          
          <a class="page-link" href="/blog">blog</a>
          
        
          
          <a class="page-link" href="/portfolio">projects</a>
          
        

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Reinforcement Learning, Teaching AI to Play</h1>
    <p class="post-meta">July 14, 2018 — 10:42 • <a href="https://martinholub.github.io/eth/code/2018/07/14/ReinforcementLearning.html#disqus_thread">0 Comments</a></p>
  </header>
  
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'] ],
      processEscapes: false,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    messageStyle: "normal",
    "HTML-CSS": {
      preferredFont: null
    },
    TeX: {
      extensions: ["AMSmath.js", "AMSsymbols.js","AMScd.js", "autobold.js", "begingroup.js", "mhchem.js"]
    }
  });
</script>
<script
  type="text/javascript"
  charset="utf-8"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
>
</script>


  <article class="post-content">
    <div class="img_row" style="width: 90%;">
  <img class="col three" src="/img/rl/breakout_head.jpg" alt="" title="Header" />
</div>

<p>In this post, I will demonstrate and explain <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">Reinforcement Learning</a> code I developed. You will learn how one can train an AI agent to master Atari games and understand the technology behind <a href="https://deepmind.com/research/alphago/">DeepMind’s AlphaGo</a>, the first computer program to defeat professional human Go player. This will be fun and, surprisingly, simple, so let’s dive in.</p>

<h2 id="openai-gym">OpenAI Gym</h2>

<p><a href="https://github.com/openai/gym">OpenAI’s <code class="language-html highlighter-rouge">gym</code></a> is a toolkit for developing and comparing reinforcement learning algorithms. <a href="https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research">Big standardized datasets</a> have proven pivotal in development of deep learning algorithms. Similarly, collection of test problems, environments, as provided by <code class="language-html highlighter-rouge">gym</code> aids in evaluation of reinforcement learning algorithms when an agent learns to take actions in response to observations from environment to “solve it”. All that comes below is enabled by the work of guys at OpenAI. Hat tip to them.</p>

<h2 id="the-cartpole-problem">The CartPole Problem</h2>

<div class="img_row" style="width: 75%;">
  <img class="col three" src="/img/rl/cartpole.gif" alt="" title="CartPole Agent" />
</div>
<div class="col three caption">
Trained agent skillfully balancing the pole on the cart
</div>

<p>The <code class="language-html highlighter-rouge">CartPole-v0</code> environment is a reinforcement learning (RL) equivalent of <em>Hello World!</em>. As such, it is sufficiently simple to get us started. It is a form of the classic control problem of inverted pendulum where we balance a pole by moving a cart it is attached to either left or right. The environment is considered solved when the average reward is greater than or equal to 195.0 over 100 consecutive trials. You can check the environment’s spec’s at its <a href="https://github.com/openai/gym/wiki/CartPole-v0">wiki</a>.</p>

<h3 id="sarsa">SARSA</h3>

<p>To train an agent for this environment, I will use the <a href="https://en.wikipedia.org/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action">SARSA</a> reinforcement learning algorithm. This is an <em>on-policy</em> learning algorithm (which means that it uses the same policy to generate both current $a_t$ as well as the next action $a_{t+1}$). The name SARSA comes from the fact that the update rule for a <em>Q-value</em> depends on the tuple $(s_t, a_t, r_t, s_{t+1}, a_{t+1})$, where $s_t$, $a_t$ and $r_t$ are state, action and reward at time $t$ respectively.</p>

<p>The concept of <em>Q-value</em> is simple one. You just need to understand that we need a value to optimize that describes the reward the agent has harvested during a period of gameplay. First, define <em>Value function</em> $V^{\pi}(s)$ which is an expected sum of discounted rewards upon starting at state $s$ and selecting actions according to policy $\pi$:</p>

<!-- https://stats.stackexchange.com/questions/326788/when-to-choose-sarsa-vs-q-learning -->
<!-- https://datascience.stackexchange.com/questions/9832/what-is-the-q-function-and-what-is-the-v-function-in-reinforcement-learning/31791#31791 -->

<script type="math/tex; mode=display">V^{\pi}(s) = E[R(s_0)+\gamma R(s_1)+\gamma^2 R(s_2) + ... | s_0=s,\pi].</script>

<p>$R(\cdot)$ gives the immediate value of reward and the discount factor $\gamma$ expresses how much we care about past values of it. <em>Q-value</em> function is then just <em>V</em> function for a particular action $a_t$ for each state $s_t$. They are related with:</p>

<script type="math/tex; mode=display">V^\pi(s) = \sum_{a \in A} \pi (a|s)  * Q^\pi(a,s).</script>

<p>Given the above definitions, the SARSA algorithm updates the <em>Q-value</em> by an error-term adjusted by the learning rate $\alpha$ as follows:</p>

<script type="math/tex; mode=display">Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha[r_{t+1}+\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t)]</script>

<p>In my implementation I <a href="http://artint.info/2e/html/ArtInt2e.Ch12.S9.SS1.html">approximate</a> the <em>Q-values</em> for each action with a linear function and select an action with $argmax(w^Ts)$.</p>

<p>Here is the implementation (full code <a href="https://gist.github.com/martinholub/c4860006d0cf3fbe87a79a054a9c98cd">here</a>):</p>

<script src="https://gist.github.com/c4860006d0cf3fbe87a79a054a9c98cd.js?file=example.py"> </script>

<h3 id="results">Results</h3>

<p>The algorithm works very well and we are able to solve the environment as early as after 60 or so episodes.</p>

<div class="img_row" style="width: 75%;">
  <img class="col three" src="/img/rl/cartpole.png" alt="" title="Cartpole" />
</div>
<div class="col three caption">
Solving the CartPole problem
</div>

<h2 id="atari-games">Atari Games</h2>

<div class="img_row" style="width: 80%;">
  <img class="col three" src="/img/rl/space_invaders.gif" alt="" title="SpaceInvaders" />
</div>
<div class="col three caption">
Example of an Atari Game (Space Invaders)
</div>

<p>After successfully training an agent on a simple problem, I jumped on something more complex. That is, teaching agent how to master <em>Atari games</em>. I focused on the game of <a href="https://gym.openai.com/envs/Breakout-v0/">Breakout</a>. This problem is very different from the previous one in that as input to our model we receive images of the game screen. Learning something from this complex input requires a model that is capable of abstracting features from it. This is why I decided to harness the power of a deep convolutional neural network.</p>

<h3 id="the-deep-model">The Deep Model</h3>

<p>For my implementation, I chose <a href="https://github.com/deepmind/dqn">the DQN model and algorithm</a> that was used by DeepMind to learn how to play Go. I extended it by few recent developments from the field of deep reinforcement learning including the <a href="https://arxiv.org/pdf/1511.06581.pdf">dueling DQN</a> and <a href="https://arxiv.org/pdf/1509.06461.pdf">double DQN</a>. I also used the <a href="https://github.com/rlcode/per">Prioritized Experience Replay Memory</a> to train the agent on samples from past to prevent it forgetting about past experiences. The full code is available at <a href="https://github.com/martinholub/demos-blogs-examples/tree/master/rl-gym/atari">my GitHub</a>.</p>

<p>Here is the model in code:</p>

<script src="https://gist.github.com/083d392d713e515fb5b96379a77670e0.js?file=model.py"> </script>

<p>You will see that the model architecture is pretty big. This is why the researchers at DeepMind had to train it for <em>50 million</em> frames to achieve great results. You will notice that I am using a <a href="https://becominghuman.ai/beat-atari-with-deep-reinforcement-learning-part-2-dqn-improvements-d3563f665a2c">custom loss function</a>. It allows me to update Q-values only for actions that were observed and also avoids the problem of weighting outliers too heavily that otherwise occurs with mean squared loss.</p>

<p>The <a href="https://en.wikipedia.org/wiki/Huber_loss">Huber loss</a> is implemented as follows:</p>

<script src="https://gist.github.com/083d392d713e515fb5b96379a77670e0.js?file=huber_loss.py"> </script>

<h3 id="epsilon-greedy-policy">Epsilon-Greedy Policy</h3>

<p>I used <code class="language-html highlighter-rouge">RMSProp</code> optimizer as it was also the one used in the original paper. Once the memory has been initialized with sufficient number of samples, then the network is retrained on each step by sampling a random batch from memory. The policy implemented is <em>Epsilon Greedy Policy</em> and it differs from the previously described SARSA algorithm in that it is implemented as an <em>off-policy</em> algorithm. That is, it chooses the next action in greedy fashion, in order to maximize the next reward.</p>

<script src="https://gist.github.com/083d392d713e515fb5b96379a77670e0.js?file=training.py"> </script>

<h3 id="checkpointing-and-visualizations">Checkpointing and Visualizations</h3>

<p>Initially, I was getting hardly any improvements, even after many hours of training. I learned the hard way that ample and frequent checkpointing, logging and visualizations are absolute musts in training deep learning models. I found it the most convenient to define these with <a href="https://keras.io/callbacks/">Keras’s <code class="language-html highlighter-rouge">callbacks</code></a> and use <a href="https://www.tensorflow.org/guide/summaries_and_tensorboard"><code class="language-html highlighter-rouge">TensorBoard</code></a> for monitoring them.  Here is an example of callbacks used to update mask of custom loss as well as to add custom visualization to TensorBoard.</p>

<script src="https://gist.github.com/083d392d713e515fb5b96379a77670e0.js?file=callbacks_examples.py"> </script>

<p>For further examples of implementing callbacks in RL, check out the <a href="https://github.com/keras-rl/keras-rl">keras-rl repo</a>.</p>

<div class="img_row">
  <img class="col three" src="/img/rl/tensorboard.png" alt="" title="TensorBoard" />
</div>
<div class="col three caption">
Example of TensorBoard visualizations showing progress on training (on x axis is number of frames).
</div>

<h3 id="results-1">Results</h3>

<p>After about 3000 episodes of training, my agent is able to play the game quite well :)</p>

<div class="img_row" style="width: 50%;">
  <img class="col three" src="/img/rl/breakout.gif" alt="" title="BreakOut" />
</div>
<div class="col three caption">
Independent AI agent playing BreakOut after 3000 episodes of training
</div>

<h2 id="conclusion">Conclusion</h2>

<p>In this post, I have demonstrated that reinforcement learning problems can be solved with reasonably simple, yet very powerful algorithms, I also pin-pointed some critical aspects of training a DL/RL model. The reinforcement learning field has made great leaps forward in past years and it appears to be staying on this track. I am personally really excited about <a href="https://ai.googleblog.com/2018/06/scalable-deep-reinforcement-learning.html">applying RL in robotics</a> and how will AI developed once it receives a physical body with means to experience and explore its environment.</p>

<p><em>Thanks for reading, everyone!</em></p>

<hr />

<h2 id="supplementary-info">Supplementary Info</h2>

<h3 id="takeaways">Takeaways</h3>

<p>Below, I am listing some important personal takeaways. You will benefit from adopting them, especially if you didn’t do much of a practical DL/RL previously.</p>

<ul>
  <li>Make sure you can fit a tiny dataset/simple problem</li>
  <li>Before adopting and adapting reference implementation, test that it works</li>
  <li>Make changes incrementally and test that they produce expected results. First get something that works, only then make it nice.</li>
  <li>For RL, you don’t need neither <a href="https://keras.io/layers/normalization/">BatchNormalization</a> nor <a href="https://keras.io/layers/core/#dropout">Dropout</a>. Regularization is in general much less important than in other settings.</li>
  <li>Visualize loss, metrics, parameter updates, Q-values, actions and more throughout the training (<a href="">Callbacks</a> and <a href="">Tensorboard</a> are your friends)</li>
  <li>Periodically visualize the agent as it plays</li>
  <li>Get proper hardware for training. Either good desktop or remote resources.</li>
</ul>

<p>I also created a <a href="https://github.com/hakimel/reveal.js">Reveal.js</a> presentation where I go through these and other highly relevant and mainly practical aspects of deep learning models and training. <a href="/data/dl-intro/index.html" target="_blank">Check it out here.</a></p>

<h3 id="references">References</h3>

<ul>
  <li><a href="http://artint.info/2e/html/ArtInt2e.Ch12.S9.SS1.html">SARSA with Linear Function Approximation</a></li>
  <li><a href="https://datascience.stackexchange.com/questions/9832/what-is-the-q-function-and-what-is-the-v-function-in-reinforcement-learning/31791#31791">Value and Q-Value</a></li>
  <li><a href="https://stats.stackexchange.com/questions/326788/when-to-choose-sarsa-vs-q-learning">Practical differneces between SARSA and Q-learning</a></li>
</ul>

  </article>

  
<!---
Call Fontawesome in the head section or in the location where you place the share bar
<link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">
--->
<hr>
<br/>
<div class="share-box center" style="width: 60%; margin: 0px auto;">
<h2 class="center">Share this:</h2>
<br/>

<a class="f" href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/eth/code/2018/07/14/ReinforcementLearning.html" onclick="window.open(this.href, 'mywin',
'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" ><i class="fa fa-facebook-official fa"></i><span> facebook</span></a>

<a class="t" href="https://twitter.com/intent/tweet?text=Reinforcement Learning, Teaching AI to Play&url=http://localhost:4000/eth/code/2018/07/14/ReinforcementLearning.html" onclick="window.open(this.href, 'mywin',
'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;"><i class="fa fa-twitter fa"></i><span> twitter</span></a>
<!---
<a class="g" href="https://plus.google.com/share?url=http://localhost:4000/eth/code/2018/07/14/ReinforcementLearning.html" onclick="window.open(this.href, 'mywin',
'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" ><i class="fa fa-google-plus fa"></i><span> google</span></a>

<a class="r" href="http://www.reddit.com/submit?url=http://localhost:4000/eth/code/2018/07/14/ReinforcementLearning.html" onclick="window.open(this.href, 'mywin',
'left=20,top=20,width=900,height=500,toolbar=1,resizable=0'); return false;" ><i class="fa fa-reddit fa"></i><span> reddit</span></a>
--->
<a class="l" href="https://www.linkedin.com/shareArticle?mini=true&url=http://localhost:4000/eth/code/2018/07/14/ReinforcementLearning.html&title=Reinforcement Learning, Teaching AI to Play&summary=&source=martinholub" onclick="window.open(this.href, 'mywin',
'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" ><i class="fa fa-linkedin fa"></i><span> linkedin</span></a>

<a class="e" href="mailto:?subject=Reinforcement Learning, Teaching AI to Play&amp;body=Check out this site http://localhost:4000/eth/code/2018/07/14/ReinforcementLearning.html"><i class="fa fa-envelope fa"></i><span> email</span></a>
</div>
<br/>
<hr>


  

<div id="disqus_thread"></div>
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'martinholub'; // required: replace example with your forum shortname
        /*var disqus_developer = 1; // Comment out when the site is live */
        var disqus_identifier = "/eth/code/2018/07/14/ReinforcementLearning.html";

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>




</div>

      </div>
    </div>

<!--
    <footer class="site-footer">

  <div class="wrapper">
  	<p>This site was built using <a href="http://jekyllrb.com" target="_blank">Jekyll</a> and is hosted on <a href="https://github.com" target="_blank">Github</a>.</p>
  </div>
</footer>

-->
  <script id="dsq-count-scr" src="//martinholub.disqus.com/count.js" async></script>
  <script src="/css/lightbox.js"></script>

  </body>

</html>
